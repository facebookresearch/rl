{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be0fafd",
   "metadata": {},
   "source": [
    "# TensorDictModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd315a",
   "metadata": {},
   "source": [
    "We recommand reading the TensorDict tutorial before going through this one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7e457-48b5-42d2-a8cf-092f0419d2d4",
   "metadata": {},
   "source": [
    "For a convenient usage of the `TensorDict` class with `nn.Module`, TorchRL provides an interface between the two named `TensorDictModule`. <br/>\n",
    "The `TensorDictModule` class is an `nn.Module` that takes a `TensorDict` as input when called. <br/>\n",
    "It is up to the user to define the keys to be read as input and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a6de9-cf97-4565-a229-c05ad18df882",
   "metadata": {},
   "source": [
    "## `TensorDictModule` by examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b0241ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchrl.data import TensorDict\n",
    "from torchrl.modules import TensorDictModule, TensorDictSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c188a",
   "metadata": {},
   "source": [
    "### Example 1: Simple usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d21a711",
   "metadata": {},
   "source": [
    "We have a `TensorDict` with 2 entries `\"a\"` and `\"b\"` but only the value associated with `\"a\"` has to be read by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f33781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
      "        a_out: Tensor(torch.Size([5, 10]), dtype=torch.float32),\n",
      "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "tensordict = TensorDict(\n",
    "    {\"a\": torch.randn(5, 3), \"b\": torch.zeros(5, 4, 3)},\n",
    "    batch_size=[5],\n",
    ")\n",
    "linear = TensorDictModule(\n",
    "    nn.Linear(3, 10), in_keys=[\"a\"], out_keys=[\"a_out\"]\n",
    ")\n",
    "linear(tensordict)\n",
    "assert (tensordict.get(\"b\") == 0).all()\n",
    "print(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00035cbd",
   "metadata": {},
   "source": [
    "### Example 2: Multiple inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a20c22",
   "metadata": {},
   "source": [
    "Suppose we have a slightly more complex network that takes 2 entries and averages them into a single output tensor. To make a `TensorDictModule` instance read multiple input values, one must register them in the `in_keys` keyword argument of the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69098393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeLinear(nn.Module):\n",
    "    def __init__(self, in_1, in_2, out):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_1, out)\n",
    "        self.linear_2 = nn.Linear(in_2, out)\n",
    "\n",
    "    def forward(self, x_1, x_2):\n",
    "        return (self.linear_1(x_1) + self.linear_2(x_2)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd686bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4]), dtype=torch.float32),\n",
       "        output: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict(\n",
    "    {\n",
    "        \"a\": torch.randn(5, 3),\n",
    "        \"b\": torch.randn(5, 4),\n",
    "    },\n",
    "    batch_size=[5],\n",
    ")\n",
    "\n",
    "mergelinear = TensorDictModule(\n",
    "    MergeLinear(3, 4, 10), in_keys=[\"a\", \"b\"], out_keys=[\"output\"]\n",
    ")\n",
    "\n",
    "mergelinear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11256ae7",
   "metadata": {},
   "source": [
    "### Example 3: Multiple outputs\n",
    "Similarly, `TensorDictModule` not only supports multiple inputs but also multiple outputs. To make a `TensorDictModule` instance write to multiple output values values, one must register them in the `out_keys` keyword argument of the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7f709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLinear(nn.Module):\n",
    "    def __init__(self, in_1, out_1, out_2):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_1, out_1)\n",
    "        self.linear_2 = nn.Linear(in_1, out_2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_1(x), self.linear_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b2b465f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        output_1: Tensor(torch.Size([5, 4]), dtype=torch.float32),\n",
       "        output_2: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3)}, batch_size=[5])\n",
    "\n",
    "splitlinear = TensorDictModule(\n",
    "    MultiHeadLinear(3, 4, 10),\n",
    "    in_keys=[\"a\"],\n",
    "    out_keys=[\"output_1\", \"output_2\"],\n",
    ")\n",
    "splitlinear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859630c3",
   "metadata": {},
   "source": [
    "As we shown previously,`TensorDictModule` can take any `nn.Module` and perform the operations on a `TensorDict`. When having multiple input keys and output keys, make sure they match the order in the module.\n",
    "`TensorDictModule` can work with `TensorDict` instances that contain more tensors than what the `in_keys` attribute indicates. Unless a `vmap` operator is used, the `TensorDict` is modified in-place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d2d2a7-6a55-4f31-972b-041be387f9df",
   "metadata": {},
   "source": [
    "### Example 4: Combining multiples `TensorDictModule` with `TensorDictSequence`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b157d5-322c-45d6-bec9-20440b78a2bf",
   "metadata": {},
   "source": [
    "To combine multiples `TensorDictModule` instances, we can use `TensorDictSequence`. We create a list where each `TensorDictModule` must be executed sequentially. `TensorDictSequence` will read and write keys to the tensordict following the sequence of modules provided.\n",
    "\n",
    "We can also gather the inputs needed by `TensorDictSequence` with the `in_keys` property, and the outputs keys are found at the `out_keys` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e36d071-df67-4232-a8a9-78e79b32fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3)}, batch_size=[5])\n",
    "\n",
    "splitlinear = TensorDictModule(\n",
    "    MultiHeadLinear(3, 4, 10),\n",
    "    in_keys=[\"a\"],\n",
    "    out_keys=[\"output_1\", \"output_2\"],\n",
    ")\n",
    "mergelinear = TensorDictModule(\n",
    "    MergeLinear(4, 10, 13),\n",
    "    in_keys=[\"output_1\", \"output_2\"],\n",
    "    out_keys=[\"output\"],\n",
    ")\n",
    "\n",
    "split_and_merge_linear = TensorDictSequence(splitlinear, mergelinear)\n",
    "\n",
    "assert split_and_merge_linear(tensordict)[\n",
    "    \"output\"\n",
    "].shape == torch.Size([5, 13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760118ea",
   "metadata": {},
   "source": [
    "### Example 5: Compatibility with functorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2718a12",
   "metadata": {},
   "source": [
    "`TensorDictModule` comes with its own `make_functional_with_buffers` method to make it functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b553bed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        output_1: Tensor(torch.Size([5, 4]), dtype=torch.float32),\n",
       "        output_2: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3)}, batch_size=[5])\n",
    "\n",
    "splitlinear = TensorDictModule(\n",
    "    MultiHeadLinear(3, 4, 10),\n",
    "    in_keys=[\"a\"],\n",
    "    out_keys=[\"output_1\", \"output_2\"],\n",
    ")\n",
    "func, (params, buffers) = splitlinear.make_functional_with_buffers()\n",
    "func(tensordict, params=params, buffers=buffers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ac0393",
   "metadata": {},
   "source": [
    "We can also use `vmap`. We can do model ensembling with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86ccb7be",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "batched == nullptr INTERNAL ASSERT FAILED at \"/private/var/folders/fn/c72nxv0x0xj4bzdgq3b0r86r0000gn/T/pip-req-build-ulv3uwfj/functorch/csrc/Interpreter.cpp\":95, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m buffers \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrandn(num_models, \u001b[38;5;241m*\u001b[39mb\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m buffers[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m### Finally we can apply the function to the stack of params and buffer using vmap\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_rl/lib/python3.9/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/pytorch/rl/torchrl/modules/tensordict_module/common.py:346\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     tensordict: _TensorDict,\n\u001b[1;32m    342\u001b[0m     tensordict_out: Optional[_TensorDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    344\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _TensorDict:\n\u001b[1;32m    345\u001b[0m     tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(tensordict\u001b[38;5;241m.\u001b[39mget(in_key, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m in_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_keys)\n\u001b[0;32m--> 346\u001b[0m     tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    348\u001b[0m         tensors \u001b[38;5;241m=\u001b[39m (tensors,)\n",
      "File \u001b[0;32m~/Documents/pytorch/rl/torchrl/modules/tensordict_module/common.py:333\u001b[0m, in \u001b[0;36mTensorDictModule._call_module\u001b[0;34m(self, tensors, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(err_msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuffers\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    328\u001b[0m     kwargs_pruned \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    329\u001b[0m         key: item\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, item \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuffers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    332\u001b[0m     }\n\u001b[0;32m--> 333\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbuffers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_pruned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/functorch/_src/vmap.py:361\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m _check_out_dims_is_int_or_int_pytree(out_dims, func)\n\u001b[1;32m    360\u001b[0m batch_size, flat_in_dims, flat_args, args_spec \u001b[38;5;241m=\u001b[39m _process_batched_inputs(in_dims, args, func)\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/functorch/_src/vmap.py:487\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 487\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_rl/lib/python3.9/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/functorch/_src/make_functional.py:282\u001b[0m, in \u001b[0;36mFunctionalModuleWithBuffers.forward\u001b[0;34m(self, params, buffers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_state \u001b[38;5;241m=\u001b[39m _swap_state(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstateless_model,\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_names_map,\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mlist\u001b[39m(params) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(buffers))\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# Remove the loaded state on self.stateless_model\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     _swap_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstateless_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_names_map, old_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_rl/lib/python3.9/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_rl/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: batched == nullptr INTERNAL ASSERT FAILED at \"/private/var/folders/fn/c72nxv0x0xj4bzdgq3b0r86r0000gn/T/pip-req-build-ulv3uwfj/functorch/csrc/Interpreter.cpp\":95, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "num_models = 10\n",
    "\n",
    "tensordict = TensorDict({\"a\": torch.randn(5, 3)}, batch_size=[5])\n",
    "\n",
    "splitlinear_models = [\n",
    "    TensorDictModule(\n",
    "        nn.Linear(3, 10), in_keys=[\"a\"], out_keys=[\"output\"]\n",
    "    )\n",
    "    for _ in range(num_models)\n",
    "]\n",
    "\n",
    "\n",
    "### Let's extract the functional version of the common model\n",
    "func = splitlinear_models[0].make_functional_with_buffers()[0]\n",
    "\n",
    "### We also extract parameters and buffers for every block and stack them together\n",
    "params, buffers = zip(\n",
    "    *[\n",
    "        splitlinear.make_functional_with_buffers()[1]\n",
    "        for splitlinear in splitlinear_models\n",
    "    ]\n",
    ")\n",
    "## For simplicity we reinit the params. In a real application you need to stack all params\n",
    "params = [torch.randn(num_models, *p.shape) for p in params[0]]\n",
    "buffers = [torch.randn(num_models, *b.shape) for b in buffers[0]]\n",
    "\n",
    "### Finally we can apply the function to the stack of params and buffer using vmap\n",
    "func(tensordict, params=params, buffers=buffers, vmap=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be6c45-10fb-4fd1-a52f-92214b76c00a",
   "metadata": {},
   "source": [
    "## Do's and don't with `TensorDictModule`\n",
    "\n",
    "Don't use `nn.Module` wrappers with `TensorDictModule` componants. This would break some of `TensorDictModule` features such as `functorch` compatibility. \n",
    "\n",
    "Don't use `nn.Sequence`, similar to nn.Module, it would break features such as `functorch` compatibility.\n",
    "\n",
    "Do use `TensorDictSequence` instead.\n",
    "\n",
    "Don't use a different name for the output of a `TensorDictModule` as the output tensordict is just the input modified in-place:\n",
    "\n",
    "```python\n",
    "tensordict = module(tensordict)  # ok!\n",
    "tensordict_out = module(tensordict)  # don't!\n",
    "```\n",
    "\n",
    "Don't use `make_functional_with_buffers` from `functorch` directly.\n",
    "\n",
    "Do use `TensorDictModule.make_functional_with_buffers` instead.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e65356-d8b3-4197-84b8-598330c1ddc8",
   "metadata": {},
   "source": [
    "## TensorDictModule for RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49a911-933c-476f-8c9a-00e006ed043c",
   "metadata": {},
   "source": [
    "In the context of RL torchrl offers a few wrappers on `TensorDictModule`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33904a6-d405-45db-a713-47493ca8ee33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `ProbabilisticTensorDictModule`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea4eead-47b4-4029-a8ff-e3c3faf51b0f",
   "metadata": {},
   "source": [
    "`ProbabilisticTDModule` is a special case of a `TensorDictModule` where the output is\n",
    "sampled given some rule, specified by the input `default_interaction_mode`\n",
    "argument and the `exploration_mode()` global function.\n",
    "\n",
    "It consists in a wrapper around another `TensorDictModule` that returns a tensordict\n",
    "updated with the distribution parameters. `ProbabilisticTensorDictModule` is\n",
    "responsible for constructing the distribution (through the `get_dist()` method)\n",
    "and/or sampling from this distribution (through a regular `__call__()` to the\n",
    "module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dd7846a-f12c-492e-a2ef-b0c67969234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict before going through module: TensorDict(\n",
      "    fields={\n",
      "        hidden: Tensor(torch.Size([3, 8]), dtype=torch.float32),\n",
      "        input: Tensor(torch.Size([3, 4]), dtype=torch.float32)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "TensorDict after going through module now as keys action, loc and scale: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        hidden: Tensor(torch.Size([3, 8]), dtype=torch.float32),\n",
      "        input: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        loc: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        sample_log_prob: Tensor(torch.Size([3, 1]), dtype=torch.float32),\n",
      "        scale: Tensor(torch.Size([3, 4]), dtype=torch.float32)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.modules import ProbabilisticTensorDictModule\n",
    "from torchrl.modules import  TanhNormal, NormalParamWrapper\n",
    "import functorch\n",
    "td = TensorDict({\"input\": torch.randn(3, 4), \"hidden\": torch.randn(3, 8)}, [3,])\n",
    "net = NormalParamWrapper(torch.nn.GRUCell(4, 8))\n",
    "module = TensorDictModule(net, in_keys=[\"input\", \"hidden\"], out_keys=[\"loc\", \"scale\"])\n",
    "td_module = ProbabilisticTensorDictModule(\n",
    "   module=module,\n",
    "   dist_param_keys=[\"loc\", \"scale\"],\n",
    "   out_key_sample=[\"action\"],\n",
    "   distribution_class=TanhNormal,\n",
    "   return_log_prob=True,\n",
    "   )\n",
    "print(f\"TensorDict before going through module: {td}\")\n",
    "td_module(td)\n",
    "print(f\"TensorDict after going through module now as keys action, loc and scale: {td}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b1caa-bcec-4317-b685-10df23352154",
   "metadata": {},
   "source": [
    "### `Actor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e139de7d-0250-49c0-b495-8b5a404821f5",
   "metadata": {},
   "source": [
    "Actor inherits from `TensorDictModule` and comes with a default value for `out_keys` of `[\"action\"]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceeade9-47f1-4e92-897a-dd226c9371a6",
   "metadata": {},
   "source": [
    "### `ProbabilisticActor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0f53e-90aa-49a9-9d8f-5a260255e556",
   "metadata": {},
   "source": [
    "General class for probabilistic actors in RL that inherits from `ProbabilisticTensorDictModule`.\n",
    "Similarly to `Actor`, it comes with default values for the `out_keys` (`[\"action\"]`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd48bb2-b93b-4766-b7a7-19d500f17e2d",
   "metadata": {},
   "source": [
    "### `ActorCriticOperator`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc42407-4e95-4bf0-8901-5d1a4e3b2044",
   "metadata": {},
   "source": [
    "Similarly, `ActorCriticOperator` inherits from `TensorDictSequence`.\n",
    "and wraps both and actor network and a value Network. \n",
    "`ActorCriticOperator` will first compute the action from the actor and then the value according to this action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b6c6035-f9cc-41e7-bf3a-f88936f93b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        observation: Tensor(torch.Size([3, 4]), dtype=torch.float32)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        hidden: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        loc: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        observation: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        sample_log_prob: Tensor(torch.Size([3, 1]), dtype=torch.float32),\n",
      "        scale: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        state_action_value: Tensor(torch.Size([3, 1]), dtype=torch.float32)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        hidden: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        loc: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        observation: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        sample_log_prob: Tensor(torch.Size([3, 1]), dtype=torch.float32),\n",
      "        scale: Tensor(torch.Size([3, 4]), dtype=torch.float32)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        hidden: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        loc: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        observation: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        sample_log_prob: Tensor(torch.Size([3, 1]), dtype=torch.float32),\n",
      "        scale: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
      "        state_action_value: Tensor(torch.Size([3, 1]), dtype=torch.float32)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.modules import (\n",
    "    MLP,\n",
    "    ActorCriticOperator,\n",
    "    NormalParamWrapper,\n",
    "    TanhNormal,\n",
    "    ValueOperator,\n",
    ")\n",
    "from torchrl.modules.tensordict_module import ProbabilisticActor\n",
    "\n",
    "module_hidden = torch.nn.Linear(4, 4)\n",
    "td_module_hidden = TensorDictModule(\n",
    "    module=module_hidden,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"hidden\"],\n",
    ")\n",
    "module_action = NormalParamWrapper(torch.nn.Linear(4, 8))\n",
    "module_action = TensorDictModule(\n",
    "    module_action, in_keys=[\"hidden\"], out_keys=[\"loc\", \"scale\"]\n",
    ")\n",
    "td_module_action = ProbabilisticActor(\n",
    "    module=module_action,\n",
    "    dist_param_keys=[\"loc\", \"scale\"],\n",
    "    out_key_sample=[\"action\"],\n",
    "    distribution_class=TanhNormal,\n",
    "    return_log_prob=True,\n",
    ")\n",
    "module_value = MLP(in_features=8, out_features=1, num_cells=[])\n",
    "td_module_value = ValueOperator(\n",
    "    module=module_value,\n",
    "    in_keys=[\"hidden\", \"action\"],\n",
    "    out_keys=[\"state_action_value\"],\n",
    ")\n",
    "td_module = ActorCriticOperator(\n",
    "    td_module_hidden, td_module_action, td_module_value\n",
    ")\n",
    "td = TensorDict(\n",
    "    {\"observation\": torch.randn(3, 4)},\n",
    "    [\n",
    "        3,\n",
    "    ],\n",
    ")\n",
    "print(td)\n",
    "td_clone = td_module(td.clone())\n",
    "print(td_clone)\n",
    "td_clone = td_module.get_policy_operator()(td.clone())\n",
    "print(td_clone)  # no value\n",
    "td_clone = td_module.get_critic_operator()(td.clone())\n",
    "print(td_clone)  # no action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304a098",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Showcase: Implementing a transformer using TensorDictModule\n",
    "To demonstrate the flexibility of `TensorDictModule`, we are going to create a transformer that reads `TensorDict` objects using `TensorDictModule`.\n",
    "\n",
    "The following figure shows the classical transformer architecture (Vaswani et al, 2017) \n",
    "\n",
    "<img src=\"./media/transformer.png\" width = 1000px/>\n",
    "\n",
    "We have let the positional encoders aside for simplicity.\n",
    "\n",
    "Let's first import the classical transformers blocks (see `src/transformer.py`for more details.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f7ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorials.src.transformer import (\n",
    "    FFN,\n",
    "    Attention,\n",
    "    SkipLayerNorm,\n",
    "    SplitHeads,\n",
    "    TokensToQKV,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3258540-acb2-4090-a374-822dfcb857bd",
   "metadata": {},
   "source": [
    "We first create the `AttentionBlockTensorDict`, the attention block using `TensorDictModule` and `TensorDictSequence`.\n",
    "\n",
    "The wiring operation that connects the modules to each other requires us to indicate which key each of them must read and write. Unlike `nn.Sequence`, a `TensorDictSequence` can read/write more than one input/output. Moreover, its components inputs need not be identical to the previous layers outputs, allowing us to code complicated neural architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb9775bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlockTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            TensorDictModule(\n",
    "                TokensToQKV(to_dim, from_dim, latent_dim),\n",
    "                in_keys=[to_name, from_name],\n",
    "                out_keys=[\"Q\", \"K\", \"V\"],\n",
    "            ),\n",
    "            TensorDictModule(\n",
    "                SplitHeads(num_heads),\n",
    "                in_keys=[\"Q\", \"K\", \"V\"],\n",
    "                out_keys=[\"Q\", \"K\", \"V\"],\n",
    "            ),\n",
    "            TensorDictModule(\n",
    "                Attention(latent_dim, to_dim),\n",
    "                in_keys=[\"Q\", \"K\", \"V\"],\n",
    "                out_keys=[\"X_out\", \"Attn\"],\n",
    "            ),\n",
    "            TensorDictModule(\n",
    "                SkipLayerNorm(to_len, to_dim),\n",
    "                in_keys=[to_name, \"X_out\"],\n",
    "                out_keys=[to_name],\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6f291",
   "metadata": {},
   "source": [
    "We build the encoder and decoder blocks that will be part of the transformer thanks to `TensorDictModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f902006d-3f89-4ea6-84e0-a193a53e42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockEncoderTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            AttentionBlockTensorDict(\n",
    "                to_name,\n",
    "                from_name,\n",
    "                to_dim,\n",
    "                to_len,\n",
    "                from_dim,\n",
    "                latent_dim,\n",
    "                num_heads,\n",
    "            ),\n",
    "            TensorDictModule(\n",
    "                FFN(to_dim, 4 * to_dim),\n",
    "                in_keys=[to_name],\n",
    "                out_keys=[\"X_out\"],\n",
    "            ),\n",
    "            TensorDictModule(\n",
    "                SkipLayerNorm(to_len, to_dim),\n",
    "                in_keys=[to_name, \"X_out\"],\n",
    "                out_keys=[to_name],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerBlockDecoderTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            AttentionBlockTensorDict(\n",
    "                to_name,\n",
    "                to_name,\n",
    "                to_dim,\n",
    "                to_len,\n",
    "                to_dim,\n",
    "                latent_dim,\n",
    "                num_heads,\n",
    "            ),\n",
    "            TransformerBlockEncoderTensorDict(\n",
    "                to_name,\n",
    "                from_name,\n",
    "                to_dim,\n",
    "                to_len,\n",
    "                from_dim,\n",
    "                latent_dim,\n",
    "                num_heads,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dbfae5",
   "metadata": {},
   "source": [
    "We create the transformer encoder and decoder.\n",
    "\n",
    "For an encoder, we just need to take the same tokens for both queries, keys and values.\n",
    "\n",
    "For a decoder, we now can extract info from `X_from` into `X_to`. `X_from` will map to queries whereas X`_from` will map to keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c6c85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            *[\n",
    "                TransformerBlockEncoderTensorDict(\n",
    "                    to_name,\n",
    "                    from_name,\n",
    "                    to_dim,\n",
    "                    to_len,\n",
    "                    from_dim,\n",
    "                    latent_dim,\n",
    "                    num_heads,\n",
    "                )\n",
    "                for _ in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerDecoderTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            *[\n",
    "                TransformerBlockDecoderTensorDict(\n",
    "                    to_name,\n",
    "                    from_name,\n",
    "                    to_dim,\n",
    "                    to_len,\n",
    "                    from_dim,\n",
    "                    latent_dim,\n",
    "                    num_heads,\n",
    "                )\n",
    "                for _ in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            TransformerEncoderTensorDict(\n",
    "                num_blocks,\n",
    "                to_name,\n",
    "                to_name,\n",
    "                to_dim,\n",
    "                to_len,\n",
    "                to_dim,\n",
    "                latent_dim,\n",
    "                num_heads,\n",
    "            ),\n",
    "            TransformerDecoderTensorDict(\n",
    "                num_blocks,\n",
    "                from_name,\n",
    "                to_name,\n",
    "                from_dim,\n",
    "                from_len,\n",
    "                to_dim,\n",
    "                latent_dim,\n",
    "                num_heads,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1b4e2-918d-40bc-a245-15be0e9cc276",
   "metadata": {},
   "source": [
    "We now test our new `TransformerTensorDict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a680452-1462-4ee6-ba04-dce0bb855870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        Attn: Tensor(torch.Size([8, 2, 10, 3]), dtype=torch.float32),\n",
       "        K: Tensor(torch.Size([8, 2, 3, 5]), dtype=torch.float32),\n",
       "        Q: Tensor(torch.Size([8, 2, 10, 5]), dtype=torch.float32),\n",
       "        V: Tensor(torch.Size([8, 2, 3, 5]), dtype=torch.float32),\n",
       "        X_decode: Tensor(torch.Size([8, 10, 6]), dtype=torch.float32),\n",
       "        X_encode: Tensor(torch.Size([8, 3, 5]), dtype=torch.float32),\n",
       "        X_out: Tensor(torch.Size([8, 10, 6]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([8]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dim = 5\n",
    "from_dim = 6\n",
    "latent_dim = 10\n",
    "to_len = 3\n",
    "from_len = 10\n",
    "batch_size = 8\n",
    "num_heads = 2\n",
    "\n",
    "tokens = TensorDict(\n",
    "    {\n",
    "        \"X_encode\": torch.randn(batch_size, to_len, to_dim),\n",
    "        \"X_decode\": torch.randn(batch_size, from_len, from_dim),\n",
    "    },\n",
    "    batch_size=[batch_size],\n",
    ")\n",
    "\n",
    "transformer = TransformerTensorDict(\n",
    "    6,\n",
    "    \"X_encode\",\n",
    "    \"X_decode\",\n",
    "    to_dim,\n",
    "    to_len,\n",
    "    from_dim,\n",
    "    latent_dim,\n",
    "    num_heads,\n",
    ")\n",
    "\n",
    "transformer(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6448dd-5d0d-43fd-9e57-a0ac3b30ecba",
   "metadata": {},
   "source": [
    "We've achieved to create a transformer with `TensorDictModule`. This shows that `TensorDictModule`is a flexible module that can implement complex operarations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08362a-8bb8-49fb-8038-1a60c5c01ea2",
   "metadata": {},
   "source": [
    "Have fun with TensorDictModule!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
