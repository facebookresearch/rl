{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86dc2115",
   "metadata": {},
   "source": [
    "# The TensorDictModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeec13d",
   "metadata": {},
   "source": [
    "Make sure to first read the tensordict tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca082a",
   "metadata": {},
   "source": [
    "How do we use the TensorDict it in pratice? We introduce the TensorDictModule. The TensorDictModule is an nn.Module that takes a TensorDict in his forward method. The user defines the keys that the module will take as an input and write the output in the same TensorDict at a given set of key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c23098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import TensorDictModule, TensorDictSequence\n",
    "from torchrl.data import TensorDict\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d8733",
   "metadata": {},
   "source": [
    "### Example: Simple Linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e8412",
   "metadata": {},
   "source": [
    "Let's imagine we have 2 entries Tensor dict, a and b and we only want to affect a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27e2c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        a_out: Tensor(torch.Size([5, 10]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3), \"b\": torch.randn(5, 4, 3)}, batch_size=[5])\n",
    "linear = TensorDictModule(nn.Linear(3, 10),in_keys=[\"a\"], out_keys=[\"a_out\"])\n",
    "linear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c4078",
   "metadata": {},
   "source": [
    "We can also do it inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9b645c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 10]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3), \"b\": torch.randn(5, 4, 3)}, batch_size=[5])\n",
    "linear = TensorDictModule(nn.Linear(3, 10),in_keys=[\"a\"], out_keys=[\"a\"])\n",
    "linear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60788b57",
   "metadata": {},
   "source": [
    "### Example: 2 input merging with 2 linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd382cb3",
   "metadata": {},
   "source": [
    "Now lets imagine a more complex network that takes 2 entries and average them into a single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9934513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeLinear(nn.Module):\n",
    "    def __init__(self, in_1, in_2, out):\n",
    "        super().__init__()\n",
    "        self.linear_1  = nn.Linear(in_1,out)\n",
    "        self.linear_2  = nn.Linear(in_2,out)\n",
    "    def forward(self, x_1, x_2):\n",
    "        return (self.linear_1(x_1) + self.linear_2(x_2))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f3173f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32),\n",
       "        c: Tensor(torch.Size([5, 4]), dtype=torch.float32),\n",
       "        output: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3), \"b\": torch.randn(5, 4, 3), \"c\":torch.randn(5, 4)}, batch_size=[5])\n",
    "mergelinear = TensorDictModule(MergeLinear(3, 4, 10),in_keys=[\"a\",\"c\"], out_keys=[\"output\"])\n",
    "mergelinear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ca7da",
   "metadata": {},
   "source": [
    "### Example: 1 input to 2 outputs linear layer\n",
    "We can also map to multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b35f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLinear(nn.Module):\n",
    "    def __init__(self, in_1, out_1, out_2):\n",
    "        super().__init__()\n",
    "        self.linear_1  = nn.Linear(in_1,out_1)\n",
    "        self.linear_2  = nn.Linear(in_1,out_2)\n",
    "    def forward(self, x):\n",
    "        return self.linear_1(x), self.linear_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28409cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32),\n",
       "        output_1: Tensor(torch.Size([5, 4]), dtype=torch.float32),\n",
       "        output_2: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3), \"b\": torch.randn(5, 4, 3)}, batch_size=[5])\n",
    "mergelinear = TensorDictModule(MultiHeadLinear(3, 4, 10),in_keys=[\"a\"], out_keys=[\"output_1\", \"output_2\"])\n",
    "mergelinear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a37df",
   "metadata": {},
   "source": [
    "As we shown previously, the TensorDictModule can take any nn.Module and perform the operations inside a TensorDict. When having multiple input keys and output keys, make sure they match the order in the module.\n",
    "The tensordictmodule allows to use only the tensors that we want and keep the output inside the same object. It can even perform the operations inplace by setting the output key to be the same as an already set key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b159e",
   "metadata": {},
   "source": [
    "### Example: A transformer with TensorDict?\n",
    "Let's attempt to create a transformer with TensorDict and TensorDictModule\n",
    "\n",
    "Disclaimer: This implementation don't claim to be \"better\" than a classical tensor-based implementation. It is just meant to showcase the TensorDictModule features.\n",
    "For simplicity we will not have positional encoders.\n",
    "\n",
    "Let's first implement the classical transformers blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6cd37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokensToQKV(nn.Module):\n",
    "    def __init__(self, to_dim, from_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(to_dim, latent_dim)\n",
    "        self.k = nn.Linear(from_dim, latent_dim)\n",
    "        self.v = nn.Linear(from_dim, latent_dim)\n",
    "    def forward(self, X_to, X_from):\n",
    "        Q = self.q(X_to)\n",
    "        K = self.k(X_from)\n",
    "        V = self.v(X_from)\n",
    "        return Q, K, V\n",
    "\n",
    "class SplitHeads(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size, to_num, latent_dim = Q.shape\n",
    "        _, from_num, _ = K.shape\n",
    "        d_tensor = latent_dim // self.num_heads\n",
    "        Q = Q.reshape(batch_size, to_num, self.num_heads, d_tensor).transpose(1, 2)\n",
    "        K = K.reshape(batch_size, from_num, self.num_heads, d_tensor).transpose(1, 2)\n",
    "        V = V.reshape(batch_size, from_num, self.num_heads, d_tensor).transpose(1, 2)\n",
    "        return Q, K, V\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, latent_dim, to_dim):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.out = nn.Linear(latent_dim, to_dim)\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size, n_heads, to_num, d_in = Q.shape\n",
    "        attn = self.softmax(Q @ K.transpose(2, 3) / d_in)\n",
    "        out = attn @ V\n",
    "        out = self.out(out.transpose(1, 2).reshape(batch_size, to_num, n_heads*d_in))\n",
    "        return out, attn\n",
    "class SkipLayerNorm(nn.Module):\n",
    "    def __init__(self, to_len, to_dim):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm((to_len, to_dim))\n",
    "    def forward(self, x_0, x_1):\n",
    "        return self.layer_norm(x_0+x_1)\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, to_dim, hidden_dim, dropout_rate = 0.2):\n",
    "        super().__init__()\n",
    "        self.FFN = nn.Sequential(\n",
    "            nn.Linear(to_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, to_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.FFN(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df2142",
   "metadata": {},
   "source": [
    "Now, we can build the TransformerBlock thanks to the TensorDictModule. Since the changes affect the tensor dict, we just need to map outputs to the right name such as it is picked up by the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8568f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockTensorDict(TensorDictSequence):\n",
    "    def __init__(self, to_name, from_name, to_dim, to_len, from_dim, latent_dim, num_heads):\n",
    "        super().__init__(\n",
    "            TensorDictModule(TokensToQKV(to_dim, from_dim, latent_dim), in_keys=[to_name, from_name], out_keys=[\"Q\", \"K\", \"V\"]),\n",
    "            TensorDictModule(SplitHeads(num_heads), in_keys=[\"Q\", \"K\", \"V\"], out_keys=[\"Q\", \"K\", \"V\"]),\n",
    "            TensorDictModule(Attention(latent_dim, to_dim), in_keys=[\"Q\", \"K\", \"V\"], out_keys=[\"X_out\",\"Attn\"]),\n",
    "            TensorDictModule(SkipLayerNorm(to_len, to_dim), in_keys=[\"X_to\", \"X_out\"], out_keys=[\"X_to\"]),\n",
    "            TensorDictModule(FFN(to_dim, 4*to_dim), in_keys=[\"X_to\"], out_keys=[\"X_out\"]),\n",
    "            TensorDictModule(SkipLayerNorm(to_len, to_dim), in_keys=[\"X_to\", \"X_out\"], out_keys=[\"X_to\"]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0ccbbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        Attn: Tensor(torch.Size([8, 2, 3, 10]), dtype=torch.float32),\n",
       "        K: Tensor(torch.Size([8, 2, 10, 5]), dtype=torch.float32),\n",
       "        Q: Tensor(torch.Size([8, 2, 3, 5]), dtype=torch.float32),\n",
       "        V: Tensor(torch.Size([8, 2, 10, 5]), dtype=torch.float32),\n",
       "        X_from: Tensor(torch.Size([8, 10, 6]), dtype=torch.float32),\n",
       "        X_out: Tensor(torch.Size([8, 3, 5]), dtype=torch.float32),\n",
       "        X_to: Tensor(torch.Size([8, 3, 5]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([8]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dim = 5\n",
    "from_dim = 6\n",
    "latent_dim = 10\n",
    "to_len = 3\n",
    "from_len = 10\n",
    "batch_size = 8\n",
    "num_heads = 2\n",
    "\n",
    "tokens = TensorDict(\n",
    "    {\n",
    "        \"X_to\": torch.randn(batch_size, to_len, to_dim),\n",
    "        \"X_from\": torch.randn(batch_size, from_len, from_dim)\n",
    "    },\n",
    "    batch_size=[batch_size]\n",
    ")\n",
    "\n",
    "transformer_block = TransformerBlockTensorDict(\n",
    "    \"X_to\",\n",
    "    \"X_from\",\n",
    "    to_dim,\n",
    "    to_len,\n",
    "    from_dim,\n",
    "    latent_dim,\n",
    "    num_heads\n",
    ")\n",
    "\n",
    "transformer_block(tokens)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12118015",
   "metadata": {},
   "source": [
    "The output of the transformer layer can now be found at tokens[\"X_to\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c28e899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1732,  1.0383, -1.5069, -0.8307, -0.5521],\n",
       "         [-0.8138,  0.8951, -0.6802, -1.3608,  0.3892],\n",
       "         [-0.3947,  0.7221, -0.4580,  1.4660,  1.9133]],\n",
       "\n",
       "        [[-0.6783,  0.5550,  0.4649,  1.5167,  1.6206],\n",
       "         [ 0.6388, -0.3218,  0.7403,  0.2852,  0.1950],\n",
       "         [-0.6361, -1.8308,  0.0806, -1.9018, -0.7281]],\n",
       "\n",
       "        [[ 0.9160, -0.4022, -0.8094,  0.6584, -1.5115],\n",
       "         [ 0.9658, -1.1464, -0.9496, -1.4521,  0.9001],\n",
       "         [ 1.2254,  1.4921, -0.1453,  0.7521, -0.4933]],\n",
       "\n",
       "        [[-0.3162,  1.2837,  0.3168,  1.7858,  0.0061],\n",
       "         [-0.1342, -0.4649, -1.2371, -0.1789, -0.4095],\n",
       "         [ 1.8135, -0.2081, -0.6934, -1.9989,  0.4353]],\n",
       "\n",
       "        [[-0.4506,  0.7247, -1.1347, -0.1918,  1.0423],\n",
       "         [-0.4250,  0.6152, -0.7718, -1.5260,  0.6411],\n",
       "         [ 2.3155,  0.8685, -0.9266,  0.1958, -0.9766]],\n",
       "\n",
       "        [[ 1.4929, -0.4170, -0.6244,  0.0319, -0.4917],\n",
       "         [-0.9035,  0.1552,  0.6767, -0.2369,  1.3653],\n",
       "         [ 0.0061, -1.9266, -1.4868,  1.3227,  1.0363]],\n",
       "\n",
       "        [[ 0.5236, -0.9703,  0.8447, -0.1412,  1.3548],\n",
       "         [ 0.3037,  1.2733, -1.3643,  0.1551,  1.8588],\n",
       "         [-1.2808, -0.5453, -0.0203, -1.3418, -0.6499]],\n",
       "\n",
       "        [[-0.8650, -0.4386, -0.3220,  1.9545, -0.8611],\n",
       "         [-1.2402,  0.2068,  1.1227, -0.1070,  2.0424],\n",
       "         [-0.4647,  0.0610,  0.5333, -0.2866, -1.3355]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"X_to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91fd810",
   "metadata": {},
   "source": [
    "We can now create a transformer easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dbdb5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads\n",
    "    ):\n",
    "        super().__init__(*[TransformerBlockTensorDict(to_name, from_name, to_dim, to_len, from_dim, latent_dim, num_heads) for _ in range(num_blocks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "317e3161",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dim = 5\n",
    "from_dim = 6\n",
    "latent_dim = 10\n",
    "to_len = 3\n",
    "from_len = 10\n",
    "batch_size = 8\n",
    "num_heads = 2\n",
    "\n",
    "tokens = TensorDict(\n",
    "    {\n",
    "        \"X_to\":torch.randn(batch_size, to_len, to_dim),\n",
    "        \"X_from\":torch.randn(batch_size, from_len, from_dim)\n",
    "    },\n",
    "    batch_size=[batch_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd80dc",
   "metadata": {},
   "source": [
    "For an encoder, we just need to take the same tokens for both queries, keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "011c42e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1218,  1.6380,  0.5283, -1.2097, -2.1976],\n",
       "         [ 0.2963,  0.8917,  0.7497, -0.1242, -0.3194],\n",
       "         [ 0.5004,  1.3847, -0.8797, -0.3203, -1.0600]],\n",
       "\n",
       "        [[ 0.3451,  1.8204,  1.1999, -1.6162, -0.2519],\n",
       "         [-0.1433,  1.0322, -0.6018, -0.6799, -0.5113],\n",
       "         [ 0.4102,  0.7934,  0.8460, -0.9515, -1.6914]],\n",
       "\n",
       "        [[-0.2757,  1.4416,  0.8119, -0.6991, -1.2287],\n",
       "         [ 1.0713,  1.3480, -0.0162, -1.2294, -2.0809],\n",
       "         [-0.1340,  0.6964,  0.6220,  0.3045, -0.6318]],\n",
       "\n",
       "        [[ 0.1715,  2.3998,  1.4253, -1.2790, -1.2869],\n",
       "         [ 0.1807,  0.6704,  0.4473, -1.1606, -0.7941],\n",
       "         [-0.0601,  0.0541,  0.4753, -0.2883, -0.9554]],\n",
       "\n",
       "        [[ 0.2194,  1.3032, -0.0246, -0.1858, -1.3101],\n",
       "         [-0.5537,  0.8703, -1.1531, -0.8894, -1.7302],\n",
       "         [ 1.3058,  0.2501,  1.4498,  1.0468, -0.5985]],\n",
       "\n",
       "        [[ 0.8845,  0.2856,  0.9751, -0.6513,  0.4614],\n",
       "         [-0.5029,  1.6529,  1.3564, -0.8560, -1.0680],\n",
       "         [-0.7042,  0.5084,  0.6723, -1.2830, -1.7314]],\n",
       "\n",
       "        [[ 0.2185,  0.6484, -1.4813, -0.9310, -0.6846],\n",
       "         [-0.0514,  0.6041,  0.8461, -0.6310, -0.9047],\n",
       "         [ 0.5254,  2.2600,  1.2691, -0.4674, -1.2203]],\n",
       "\n",
       "        [[ 1.1002,  1.4143, -1.1888, -0.1106, -1.7328],\n",
       "         [ 0.2170,  0.5818,  0.2111,  0.4387, -1.0139],\n",
       "         [-0.4513,  1.5186,  1.0518, -1.1227, -0.9134]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder = TransformerTensorDict(\n",
    "    6,\n",
    "    \"X_to\",\n",
    "    \"X_to\",\n",
    "    to_dim,\n",
    "    to_len,\n",
    "    to_dim,\n",
    "    latent_dim,\n",
    "    num_heads\n",
    ")\n",
    "\n",
    "transformer_encoder(tokens)\n",
    "tokens[\"X_to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d709b2",
   "metadata": {},
   "source": [
    "For a decoder, we now can extract info from X_from into X_to. X_to will map to queries whereas X_from will map to keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9ed7016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5958,  1.9269,  1.4385, -1.7099, -1.2024],\n",
       "         [-0.1923,  0.6554,  0.8646, -0.9089,  0.4366],\n",
       "         [-0.1544,  0.5690,  0.1171, -1.3729,  0.1287]],\n",
       "\n",
       "        [[-0.6611,  1.6326,  0.9590, -1.3373,  0.1869],\n",
       "         [-0.5724,  0.9246,  0.3437, -1.2203, -0.2542],\n",
       "         [-0.2347,  0.9846,  1.5354, -1.5329, -0.7540]],\n",
       "\n",
       "        [[-1.0947,  1.5092,  1.3402, -1.0084, -0.4252],\n",
       "         [-0.3901,  1.3286,  1.2193, -1.4316, -1.3246],\n",
       "         [-0.7211,  0.7870,  0.2914, -0.2573,  0.1774]],\n",
       "\n",
       "        [[-1.0461,  2.1735,  1.8569, -0.7953, -0.6545],\n",
       "         [-0.3190,  0.8452,  0.8193, -1.0471, -0.7304],\n",
       "         [-0.2703,  0.0935,  0.5782, -0.6885, -0.8154]],\n",
       "\n",
       "        [[-0.3828,  1.8378,  0.1272, -0.4188, -0.1151],\n",
       "         [-0.9871,  0.3603, -1.4740, -1.2111, -1.6092],\n",
       "         [ 0.2361,  0.9122,  1.5031,  0.4612,  0.7603]],\n",
       "\n",
       "        [[ 0.1466,  0.6786,  1.1725, -0.5156,  0.1942],\n",
       "         [-0.9457,  1.7856,  1.7606, -0.9622, -0.9245],\n",
       "         [-0.8863,  0.1064,  0.6751, -1.0641, -1.2214]],\n",
       "\n",
       "        [[-0.1457,  0.3580, -0.4237, -0.6528, -0.9734],\n",
       "         [-0.6525,  0.8373,  0.8691, -0.7156, -0.2167],\n",
       "         [-0.8809,  2.1679,  2.0259, -0.8261, -0.7707]],\n",
       "\n",
       "        [[ 0.1682,  1.5683, -1.0913, -0.9862, -0.5086],\n",
       "         [-0.1570,  0.8816,  0.4636, -0.5736, -0.1888],\n",
       "         [-0.9000,  1.8169,  1.4407, -1.5377, -0.3962]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_decoder = TransformerTensorDict(\n",
    "    6,\n",
    "    \"X_to\",\n",
    "    \"X_from\",\n",
    "    to_dim,\n",
    "    to_len,\n",
    "    from_dim,\n",
    "    latent_dim,\n",
    "    num_heads\n",
    ")\n",
    "\n",
    "transformer_decoder(tokens)\n",
    "tokens[\"X_to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef98efd7",
   "metadata": {},
   "source": [
    "Now we can look at both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43888021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerTensorDict(\n",
       "    module=ModuleList(\n",
       "      (0): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_to', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (1): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_to', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (2): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_to', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (3): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_to', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (4): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_to', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (5): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_to', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "    ), \n",
       "    device=cpu, \n",
       "    in_keys=['X_to', 'X_to', 'X_to'], \n",
       "    out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c619bb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerTensorDict(\n",
       "    module=ModuleList(\n",
       "      (0): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_from'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (1): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_from'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (2): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_from'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (3): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_from'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (4): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_from'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (5): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_from'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "    ), \n",
       "    device=cpu, \n",
       "    in_keys=['X_to', 'X_from', 'X_to', 'X_from', 'X_from', 'X_from', 'X_from', 'X_from'], \n",
       "    out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900b6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
