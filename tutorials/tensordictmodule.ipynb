{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be0fafd",
   "metadata": {},
   "source": [
    "# TensorDictModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd315a",
   "metadata": {},
   "source": [
    "We recommand reading the TensorDict tutorial before going through this one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652352c",
   "metadata": {},
   "source": [
    "For a convenient usage of the `TensorDict` class with `nn.Module`, TorchRL provides an interface between the two named `TensorDictModule`. <br/>\n",
    "The `TensorDictModule` class is an `nn.Module` that takes a `TensorDict` as input when called. <br/>\n",
    "It is up to the user to define the keys to be read as input and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a6de9-cf97-4565-a229-c05ad18df882",
   "metadata": {},
   "source": [
    "## `TensorDictModule` by examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b0241ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchrl.data import TensorDict\n",
    "from torchrl.modules import TensorDictModule, TensorDictSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c188a",
   "metadata": {},
   "source": [
    "### Example 1: Simple usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d21a711",
   "metadata": {},
   "source": [
    "Let's suppose we have `TensorDict` with 2 entries `\"a\"` and `\"b\"` but only the value associated with `\"a\"` has to be read by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f33781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
      "        a_out: Tensor(torch.Size([5, 10]), dtype=torch.float32),\n",
      "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "tensordict = TensorDict(\n",
    "    {\"a\": torch.randn(5, 3), \"b\": torch.zeros(5, 4, 3)},\n",
    "    batch_size=[5],\n",
    ")\n",
    "linear = TensorDictModule(\n",
    "    nn.Linear(3, 10), in_keys=[\"a\"], out_keys=[\"a_out\"]\n",
    ")\n",
    "linear(tensordict)\n",
    "assert (tensordict.get(\"b\") == 0).all()\n",
    "print(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00035cbd",
   "metadata": {},
   "source": [
    "### Example 2: Multiple inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a20c22",
   "metadata": {},
   "source": [
    "Suppose we have a slightly more complex network that takes 2 entries and averages them into a single output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69098393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeLinear(nn.Module):\n",
    "    def __init__(self, in_1, in_2, out):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_1, out)\n",
    "        self.linear_2 = nn.Linear(in_2, out)\n",
    "\n",
    "    def forward(self, x_1, x_2):\n",
    "        return (self.linear_1(x_1) + self.linear_2(x_2)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dd686bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4]), dtype=torch.float32),\n",
       "        output: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict(\n",
    "    {\n",
    "        \"a\": torch.randn(5, 3),\n",
    "        \"b\": torch.randn(5, 4),\n",
    "    },\n",
    "    batch_size=[5],\n",
    ")\n",
    "\n",
    "mergelinear = TensorDictModule(\n",
    "    MergeLinear(3, 4, 10), in_keys=[\"a\", \"b\"], out_keys=[\"output\"]\n",
    ")\n",
    "\n",
    "mergelinear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11256ae7",
   "metadata": {},
   "source": [
    "### Example 3: Multiple outputs\n",
    "TensorDictModule not only supports multiple inputs but also multiple outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7f709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLinear(nn.Module):\n",
    "    def __init__(self, in_1, out_1, out_2):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_1, out_1)\n",
    "        self.linear_2 = nn.Linear(in_1, out_2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_1(x), self.linear_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b2b465f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        output_1: Tensor(torch.Size([5, 4]), dtype=torch.float32),\n",
       "        output_2: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3)}, batch_size=[5])\n",
    "\n",
    "splitlinear = TensorDictModule(\n",
    "    MultiHeadLinear(3, 4, 10),\n",
    "    in_keys=[\"a\"],\n",
    "    out_keys=[\"output_1\", \"output_2\"],\n",
    ")\n",
    "splitlinear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859630c3",
   "metadata": {},
   "source": [
    "As we shown previously, the `TensorDictModule` can take any `nn.Module` and perform the operations on a `TensorDict`. When having multiple input keys and output keys, make sure they match the order in the module.\n",
    "`TensorDictModule` can work with `TensorDict` instances that contain more tensors than what the `in_keys` attribute indicates. Unless a `vmap` operator is used, the `TensorDict` is modified in-place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d2d2a7-6a55-4f31-972b-041be387f9df",
   "metadata": {},
   "source": [
    "### Example 4: Combining multiples `TensorDictModule` with `TensorDictSequence`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b157d5-322c-45d6-bec9-20440b78a2bf",
   "metadata": {},
   "source": [
    "To combine multiples `TensorDictModule`instances, we can une `TensorDictSequence`. This block will take the input of the n-1th `TensorDictModule` in a list and feed it to the nth `TensorDictModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e36d071-df67-4232-a8a9-78e79b32fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3)}, batch_size=[5])\n",
    "\n",
    "splitlinear = TensorDictModule(\n",
    "    MultiHeadLinear(3, 4, 10),\n",
    "    in_keys=[\"a\"],\n",
    "    out_keys=[\"output_1\", \"output_2\"],\n",
    ")\n",
    "mergelinear = TensorDictModule(\n",
    "    MergeLinear(4, 10, 13), in_keys=[\"output_1\", \"output_2\"], out_keys=[\"output\"]\n",
    ")\n",
    "\n",
    "split_and_merge_linear = TensorDictSequence(splitlinear, mergelinear)\n",
    "\n",
    "assert split_and_merge_linear(tensordict)['output'].shape == torch.Size([5, 13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760118ea",
   "metadata": {},
   "source": [
    "### Example 5: Compatibility with functorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2718a12",
   "metadata": {},
   "source": [
    "TensorDictModule is compatible with functorch. We can use make_functional_with_buffers on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b553bed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        output_1: Tensor(torch.Size([5, 4]), dtype=torch.float32),\n",
       "        output_2: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3)}, batch_size=[5])\n",
    "\n",
    "splitlinear = TensorDictModule(\n",
    "    MultiHeadLinear(3, 4, 10),\n",
    "    in_keys=[\"a\"],\n",
    "    out_keys=[\"output_1\", \"output_2\"],\n",
    ")\n",
    "func, (params, buffers) = splitlinear.make_functional_with_buffers()\n",
    "func(tensordict, params=params, buffers=buffers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ac0393",
   "metadata": {},
   "source": [
    "We can also use vmap. Let's do some model ensembling with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86ccb7be",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "batched == nullptr INTERNAL ASSERT FAILED at \"/private/var/folders/fn/c72nxv0x0xj4bzdgq3b0r86r0000gn/T/pip-req-build-i1bssf2g/functorch/csrc/Interpreter.cpp\":95, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m params \u001b[38;5;241m=\u001b[39m transpose_stack(params)\n\u001b[1;32m     29\u001b[0m buffers \u001b[38;5;241m=\u001b[39m transpose_stack(buffers)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_rl/lib/python3.9/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/pytorch/rl/torchrl/modules/tensordict_module/common.py:346\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     tensordict: _TensorDict,\n\u001b[1;32m    342\u001b[0m     tensordict_out: Optional[_TensorDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    344\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _TensorDict:\n\u001b[1;32m    345\u001b[0m     tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(tensordict\u001b[38;5;241m.\u001b[39mget(in_key, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m in_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_keys)\n\u001b[0;32m--> 346\u001b[0m     tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    348\u001b[0m         tensors \u001b[38;5;241m=\u001b[39m (tensors,)\n",
      "File \u001b[0;32m~/Documents/pytorch/rl/torchrl/modules/tensordict_module/common.py:333\u001b[0m, in \u001b[0;36mTensorDictModule._call_module\u001b[0;34m(self, tensors, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(err_msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuffers\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    328\u001b[0m     kwargs_pruned \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    329\u001b[0m         key: item\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, item \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuffers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    332\u001b[0m     }\n\u001b[0;32m--> 333\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbuffers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_pruned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/functorch/_src/vmap.py:361\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m _check_out_dims_is_int_or_int_pytree(out_dims, func)\n\u001b[1;32m    360\u001b[0m batch_size, flat_in_dims, flat_args, args_spec \u001b[38;5;241m=\u001b[39m _process_batched_inputs(in_dims, args, func)\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/functorch/_src/vmap.py:487\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 487\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_rl/lib/python3.9/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/functorch/_src/make_functional.py:282\u001b[0m, in \u001b[0;36mFunctionalModuleWithBuffers.forward\u001b[0;34m(self, params, buffers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_state \u001b[38;5;241m=\u001b[39m _swap_state(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstateless_model,\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_names_map,\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mlist\u001b[39m(params) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(buffers))\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# Remove the loaded state on self.stateless_model\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     _swap_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstateless_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_names_map, old_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_rl/lib/python3.9/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_rl/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: batched == nullptr INTERNAL ASSERT FAILED at \"/private/var/folders/fn/c72nxv0x0xj4bzdgq3b0r86r0000gn/T/pip-req-build-i1bssf2g/functorch/csrc/Interpreter.cpp\":95, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "num_models = 10\n",
    "\n",
    "tensordict = TensorDict({\"a\": torch.randn(5, 3)}, batch_size=[5])\n",
    "\n",
    "splitlinear_models = [\n",
    "    TensorDictModule(\n",
    "        nn.Linear(3, 10), in_keys=[\"a\"], out_keys=[\"output\"]\n",
    "    )\n",
    "    for _ in range(num_models)\n",
    "]\n",
    "\n",
    "\n",
    "def transpose_stack(tuple_of_tuple_of_tensors):\n",
    "    tuple_of_tuple_of_tensors = tuple(zip(*tuple_of_tuple_of_tensors))\n",
    "    results = tuple(\n",
    "        torch.stack(shards).detach()\n",
    "        for shards in tuple_of_tuple_of_tensors\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "func = splitlinear_models[0].make_functional_with_buffers()[0]\n",
    "params, buffers = zip(\n",
    "    *[\n",
    "        splitlinear.make_functional_with_buffers()[1]\n",
    "        for splitlinear in splitlinear_models\n",
    "    ]\n",
    ")\n",
    "params = transpose_stack(params)\n",
    "buffers = transpose_stack(buffers)\n",
    "func(tensordict, params=params, buffers=buffers, vmap=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ed39eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        output: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functorch import make_functional_with_buffers\n",
    "\n",
    "tensordict = TensorDict({\"a\": torch.randn(5, 3)}, batch_size=[5])\n",
    "\n",
    "splitlinear = TensorDictModule(\n",
    "    nn.Linear(3, 10), in_keys=[\"a\"], out_keys=[\"output\"]\n",
    ")\n",
    "func, param, buffers = make_functional_with_buffers(splitlinear)\n",
    "func(param, buffers, tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304a098",
   "metadata": {},
   "source": [
    "### Example 6: Implementing a transformer using TensorDictModule\n",
    "We can easily create a transformer that reads TensorDict objects using TensorDictModule.\n",
    "\n",
    "The following figure shows the classical transformer architecture (Vaswani et al, 2017) \n",
    "\n",
    "<img src=\"./media/transformer.png\" width = 1000px/>\n",
    "\n",
    "We have let the positional encoders aside for simplicity.\n",
    "\n",
    "Let's first implement the classical transformers blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1f7ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokensToQKV(nn.Module):\n",
    "    def __init__(self, to_dim, from_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(to_dim, latent_dim)\n",
    "        self.k = nn.Linear(from_dim, latent_dim)\n",
    "        self.v = nn.Linear(from_dim, latent_dim)\n",
    "\n",
    "    def forward(self, X_to, X_from):\n",
    "        Q = self.q(X_to)\n",
    "        K = self.k(X_from)\n",
    "        V = self.v(X_from)\n",
    "        return Q, K, V\n",
    "\n",
    "\n",
    "class SplitHeads(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size, to_num, latent_dim = Q.shape\n",
    "        _, from_num, _ = K.shape\n",
    "        d_tensor = latent_dim // self.num_heads\n",
    "        Q = Q.reshape(\n",
    "            batch_size, to_num, self.num_heads, d_tensor\n",
    "        ).transpose(1, 2)\n",
    "        K = K.reshape(\n",
    "            batch_size, from_num, self.num_heads, d_tensor\n",
    "        ).transpose(1, 2)\n",
    "        V = V.reshape(\n",
    "            batch_size, from_num, self.num_heads, d_tensor\n",
    "        ).transpose(1, 2)\n",
    "        return Q, K, V\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, latent_dim, to_dim):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.out = nn.Linear(latent_dim, to_dim)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size, n_heads, to_num, d_in = Q.shape\n",
    "        attn = self.softmax(Q @ K.transpose(2, 3) / d_in)\n",
    "        out = attn @ V\n",
    "        out = self.out(\n",
    "            out.transpose(1, 2).reshape(\n",
    "                batch_size, to_num, n_heads * d_in\n",
    "            )\n",
    "        )\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "class SkipLayerNorm(nn.Module):\n",
    "    def __init__(self, to_len, to_dim):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm((to_len, to_dim))\n",
    "\n",
    "    def forward(self, x_0, x_1):\n",
    "        return self.layer_norm(x_0 + x_1)\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, to_dim, hidden_dim, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.FFN = nn.Sequential(\n",
    "            nn.Linear(to_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, to_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.FFN(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6f291",
   "metadata": {},
   "source": [
    "We can build the encoder and decoder blocks that will be part of the transformer thanks to the `TensorDictModule`. Since the changes affect the `TensorDict`, we just need to map outputs to the right name such as it is picked up by the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb9775bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlockTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            TensorDictModule(\n",
    "                TokensToQKV(to_dim, from_dim, latent_dim),\n",
    "                in_keys=[to_name, from_name],\n",
    "                out_keys=[\"Q\", \"K\", \"V\"],\n",
    "            ),\n",
    "            TensorDictModule(\n",
    "                SplitHeads(num_heads),\n",
    "                in_keys=[\"Q\", \"K\", \"V\"],\n",
    "                out_keys=[\"Q\", \"K\", \"V\"],\n",
    "            ),\n",
    "            TensorDictModule(\n",
    "                Attention(latent_dim, to_dim),\n",
    "                in_keys=[\"Q\", \"K\", \"V\"],\n",
    "                out_keys=[\"X_out\", \"Attn\"],\n",
    "            ),\n",
    "            TensorDictModule(\n",
    "                SkipLayerNorm(to_len, to_dim),\n",
    "                in_keys=[to_name, \"X_out\"],\n",
    "                out_keys=[to_name],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerBlockEncoderTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            AttentionBlockTensorDict(\n",
    "                to_name,\n",
    "                from_name,\n",
    "                to_dim,\n",
    "                to_len,\n",
    "                from_dim,\n",
    "                latent_dim,\n",
    "                num_heads,\n",
    "            ),\n",
    "            TensorDictModule(\n",
    "                FFN(to_dim, 4 * to_dim),\n",
    "                in_keys=[to_name],\n",
    "                out_keys=[\"X_out\"],\n",
    "            ),\n",
    "            TensorDictModule(\n",
    "                SkipLayerNorm(to_len, to_dim),\n",
    "                in_keys=[to_name, \"X_out\"],\n",
    "                out_keys=[to_name],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerBlockDecoderTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            AttentionBlockTensorDict(\n",
    "                to_name,\n",
    "                to_name,\n",
    "                to_dim,\n",
    "                to_len,\n",
    "                to_dim,\n",
    "                latent_dim,\n",
    "                num_heads,\n",
    "            ),\n",
    "            TransformerBlockEncoderTensorDict(\n",
    "                to_name,\n",
    "                from_name,\n",
    "                to_dim,\n",
    "                to_len,\n",
    "                from_dim,\n",
    "                latent_dim,\n",
    "                num_heads,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9601f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        Attn: Tensor(torch.Size([8, 2, 3, 10]), dtype=torch.float32),\n",
       "        K: Tensor(torch.Size([8, 2, 10, 5]), dtype=torch.float32),\n",
       "        Q: Tensor(torch.Size([8, 2, 3, 5]), dtype=torch.float32),\n",
       "        V: Tensor(torch.Size([8, 2, 10, 5]), dtype=torch.float32),\n",
       "        X_from: Tensor(torch.Size([8, 10, 6]), dtype=torch.float32),\n",
       "        X_out: Tensor(torch.Size([8, 3, 5]), dtype=torch.float32),\n",
       "        X_to: Tensor(torch.Size([8, 3, 5]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([8]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dim = 5\n",
    "from_dim = 6\n",
    "latent_dim = 10\n",
    "to_len = 3\n",
    "from_len = 10\n",
    "batch_size = 8\n",
    "num_heads = 2\n",
    "\n",
    "tokens = TensorDict(\n",
    "    {\n",
    "        \"X_to\": torch.randn(batch_size, to_len, to_dim),\n",
    "        \"X_from\": torch.randn(batch_size, from_len, from_dim),\n",
    "    },\n",
    "    batch_size=[batch_size],\n",
    ")\n",
    "\n",
    "transformer_block = AttentionBlockTensorDict(\n",
    "    \"X_to\", \"X_from\", to_dim, to_len, from_dim, latent_dim, num_heads\n",
    ")\n",
    "\n",
    "transformer_block(tokens)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2b6cb",
   "metadata": {},
   "source": [
    "The output of the attention can now be found at `tokens[\"X_to\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dbfae5",
   "metadata": {},
   "source": [
    "We create the transformer encoder and decoder.\n",
    "\n",
    "For an encoder, we just need to take the same tokens for both queries, keys and values.\n",
    "\n",
    "For a decoder, we now can extract info from `X_from` into `X_to`. `X_from` will map to queries whereas X`_from` will map to keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c6c85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            *[\n",
    "                TransformerBlockEncoderTensorDict(\n",
    "                    to_name,\n",
    "                    from_name,\n",
    "                    to_dim,\n",
    "                    to_len,\n",
    "                    from_dim,\n",
    "                    latent_dim,\n",
    "                    num_heads,\n",
    "                )\n",
    "                for _ in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerDecoderTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            *[\n",
    "                TransformerBlockDecoderTensorDict(\n",
    "                    to_name,\n",
    "                    from_name,\n",
    "                    to_dim,\n",
    "                    to_len,\n",
    "                    from_dim,\n",
    "                    latent_dim,\n",
    "                    num_heads,\n",
    "                )\n",
    "                for _ in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            TransformerEncoderTensorDict(\n",
    "                num_blocks,\n",
    "                to_name,\n",
    "                to_name,\n",
    "                to_dim,\n",
    "                to_len,\n",
    "                to_dim,\n",
    "                latent_dim,\n",
    "                num_heads,\n",
    "            ),\n",
    "            TransformerDecoderTensorDict(\n",
    "                num_blocks,\n",
    "                from_name,\n",
    "                to_name,\n",
    "                from_dim,\n",
    "                from_len,\n",
    "                to_dim,\n",
    "                latent_dim,\n",
    "                num_heads,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09fa9f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        Attn: Tensor(torch.Size([8, 2, 10, 3]), dtype=torch.float32),\n",
       "        K: Tensor(torch.Size([8, 2, 3, 5]), dtype=torch.float32),\n",
       "        Q: Tensor(torch.Size([8, 2, 10, 5]), dtype=torch.float32),\n",
       "        V: Tensor(torch.Size([8, 2, 3, 5]), dtype=torch.float32),\n",
       "        X_decode: Tensor(torch.Size([8, 10, 6]), dtype=torch.float32),\n",
       "        X_encode: Tensor(torch.Size([8, 3, 5]), dtype=torch.float32),\n",
       "        X_out: Tensor(torch.Size([8, 10, 6]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([8]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dim = 5\n",
    "from_dim = 6\n",
    "latent_dim = 10\n",
    "to_len = 3\n",
    "from_len = 10\n",
    "batch_size = 8\n",
    "num_heads = 2\n",
    "\n",
    "tokens = TensorDict(\n",
    "    {\n",
    "        \"X_encode\": torch.randn(batch_size, to_len, to_dim),\n",
    "        \"X_decode\": torch.randn(batch_size, from_len, from_dim),\n",
    "    },\n",
    "    batch_size=[batch_size],\n",
    ")\n",
    "\n",
    "transformer = TransformerTensorDict(\n",
    "    6,\n",
    "    \"X_encode\",\n",
    "    \"X_decode\",\n",
    "    to_dim,\n",
    "    to_len,\n",
    "    from_dim,\n",
    "    latent_dim,\n",
    "    num_heads,\n",
    ")\n",
    "\n",
    "transformer(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6448dd-5d0d-43fd-9e57-a0ac3b30ecba",
   "metadata": {},
   "source": [
    "We've achieved to create a transformer with `TensorDictModule`. This shows that `TensorDictModule`is a flexible module that can implement complex operarations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e65356-d8b3-4197-84b8-598330c1ddc8",
   "metadata": {},
   "source": [
    "## TensorDictModule for RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49a911-933c-476f-8c9a-00e006ed043c",
   "metadata": {},
   "source": [
    "In the context of RL torchrl offers a few wrappers on `TensorDictModule`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33904a6-d405-45db-a713-47493ca8ee33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `ProbabilisticTensorDictModule`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea4eead-47b4-4029-a8ff-e3c3faf51b0f",
   "metadata": {},
   "source": [
    "`ProbabilisticTDModule` is a special case of a `TensorDictModule` where the output is\n",
    "sampled given some rule, specified by the input `default_interaction_mode`\n",
    "argument and the `exploration_mode()` global function.\n",
    "\n",
    "It consists in a wrapper around another `TensorDictModule` that returns a tensordict\n",
    "updated with the distribution parameters. `ProbabilisticTensorDictModule` is\n",
    "responsible for constructing the distribution (through the `get_dist()` method)\n",
    "and/or sampling from this distribution (through a regular `__call__()` to the\n",
    "module)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b1caa-bcec-4317-b685-10df23352154",
   "metadata": {},
   "source": [
    "### `Actor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e139de7d-0250-49c0-b495-8b5a404821f5",
   "metadata": {},
   "source": [
    "Actor inherits from `TensorDictModule` and comes with a default value for `out_keys` of `[\"action\"]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceeade9-47f1-4e92-897a-dd226c9371a6",
   "metadata": {},
   "source": [
    "### `ProbabilisticActor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0f53e-90aa-49a9-9d8f-5a260255e556",
   "metadata": {},
   "source": [
    "General class for probabilistic actors in RL that inherits from `ProbabilisticTensorDictModule`.\n",
    "Similarly to `Actor`, it comes with default values for the `out_keys` (`[\"action\"]`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd48bb2-b93b-4766-b7a7-19d500f17e2d",
   "metadata": {},
   "source": [
    "### `ActorCriticOperator`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc42407-4e95-4bf0-8901-5d1a4e3b2044",
   "metadata": {},
   "source": [
    "Similarly, `ActorCriticOperator` inherits from `TensorDictSequence`.\n",
    "and wraps both and actor network and a value Network. \n",
    "`ActorCriticOperator` will first compute the action from the actor and then the value according to this action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08362a-8bb8-49fb-8038-1a60c5c01ea2",
   "metadata": {},
   "source": [
    "Have fun with TensorDictModule!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
