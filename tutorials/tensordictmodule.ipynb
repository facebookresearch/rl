{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b194ee2",
   "metadata": {},
   "source": [
    "# The TensorDictModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc268c66",
   "metadata": {},
   "source": [
    "Make sure to first read the tensordict tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16eeedd",
   "metadata": {},
   "source": [
    "How do we use the TensorDict it in pratice? We introduce the TensorDictModule. The TensorDictModule is an nn.Module that takes a TensorDict in his forward method. The user defines the keys that the module will take as an input and write the output in the same TensorDict at a given set of key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e43fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import TensorDictModule, TensorDictSequence\n",
    "from torchrl.data import TensorDict\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46be707",
   "metadata": {},
   "source": [
    "### Example: Simple Linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e3f58",
   "metadata": {},
   "source": [
    "Let's imagine we have 2 entries Tensor dict, a and b and we only want to affect a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c688617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        a_out: Tensor(torch.Size([5, 10]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3), \"b\": torch.randn(5, 4, 3)}, batch_size=[5])\n",
    "linear = TensorDictModule(nn.Linear(3, 10),in_keys=[\"a\"], out_keys=[\"a_out\"])\n",
    "linear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71e15f",
   "metadata": {},
   "source": [
    "We can also do it inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a33bfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 10]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3), \"b\": torch.randn(5, 4, 3)}, batch_size=[5])\n",
    "linear = TensorDictModule(nn.Linear(3, 10),in_keys=[\"a\"], out_keys=[\"a\"])\n",
    "linear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963909a1",
   "metadata": {},
   "source": [
    "### Example: 2 input merging with 2 linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a6f397",
   "metadata": {},
   "source": [
    "Now lets imagine a more complex network that takes 2 entries and average them into a single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f0e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeLinear(nn.Module):\n",
    "    def __init__(self, in_1, in_2, out):\n",
    "        super().__init__()\n",
    "        self.linear_1  = nn.Linear(in_1,out)\n",
    "        self.linear_2  = nn.Linear(in_2,out)\n",
    "    def forward(self, x_1, x_2):\n",
    "        return (self.linear_1(x_1) + self.linear_2(x_2))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d3ddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32),\n",
       "        c: Tensor(torch.Size([5, 4]), dtype=torch.float32),\n",
       "        output: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3), \"b\": torch.randn(5, 4, 3), \"c\":torch.randn(5, 4)}, batch_size=[5])\n",
    "mergelinear = TensorDictModule(MergeLinear(3, 4, 10),in_keys=[\"a\",\"c\"], out_keys=[\"output\"])\n",
    "mergelinear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41e873",
   "metadata": {},
   "source": [
    "### Example: 1 input to 2 outputs linear layer\n",
    "We can also map to multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86d4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLinear(nn.Module):\n",
    "    def __init__(self, in_1, out_1, out_2):\n",
    "        super().__init__()\n",
    "        self.linear_1  = nn.Linear(in_1,out_1)\n",
    "        self.linear_2  = nn.Linear(in_1,out_2)\n",
    "    def forward(self, x):\n",
    "        return self.linear_1(x), self.linear_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f132410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32),\n",
       "        output_1: Tensor(torch.Size([5, 4]), dtype=torch.float32),\n",
       "        output_2: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3), \"b\": torch.randn(5, 4, 3)}, batch_size=[5])\n",
    "mergelinear = TensorDictModule(MultiHeadLinear(3, 4, 10),in_keys=[\"a\"], out_keys=[\"output_1\", \"output_2\"])\n",
    "mergelinear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b852dff",
   "metadata": {},
   "source": [
    "As we shown previously, the TensorDictModule can take any nn.Module and perform the operations inside a TensorDict. When having multiple input keys and output keys, make sure they match the order in the module.\n",
    "The tensordictmodule allows to use only the tensors that we want and keep the output inside the same object. It can even perform the operations inplace by setting the output key to be the same as an already set key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a538a",
   "metadata": {},
   "source": [
    "### Example: A transformer with TensorDict?\n",
    "Let's attempt to create a transformer with TensorDict and TensorDictModule.\n",
    "\n",
    "Here's a diagram that sums up the architecture:\n",
    "\n",
    "<img src=\"./media/transformer.png\" width = 1000px/>\n",
    "Disclaimer: This implementation don't claim to be \"better\" than a classical tensor-based implementation. It is just meant to showcase the TensorDictModule features.\n",
    "For simplicity we will not have positional encoders.\n",
    "\n",
    "Let's first implement the classical transformers blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba74542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokensToQKV(nn.Module):\n",
    "    def __init__(self, to_dim, from_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(to_dim, latent_dim)\n",
    "        self.k = nn.Linear(from_dim, latent_dim)\n",
    "        self.v = nn.Linear(from_dim, latent_dim)\n",
    "    def forward(self, X_to, X_from):\n",
    "        Q = self.q(X_to)\n",
    "        K = self.k(X_from)\n",
    "        V = self.v(X_from)\n",
    "        return Q, K, V\n",
    "\n",
    "class SplitHeads(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size, to_num, latent_dim = Q.shape\n",
    "        _, from_num, _ = K.shape\n",
    "        d_tensor = latent_dim // self.num_heads\n",
    "        Q = Q.reshape(batch_size, to_num, self.num_heads, d_tensor).transpose(1, 2)\n",
    "        K = K.reshape(batch_size, from_num, self.num_heads, d_tensor).transpose(1, 2)\n",
    "        V = V.reshape(batch_size, from_num, self.num_heads, d_tensor).transpose(1, 2)\n",
    "        return Q, K, V\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, latent_dim, to_dim):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.out = nn.Linear(latent_dim, to_dim)\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size, n_heads, to_num, d_in = Q.shape\n",
    "        attn = self.softmax(Q @ K.transpose(2, 3) / d_in)\n",
    "        out = attn @ V\n",
    "        out = self.out(out.transpose(1, 2).reshape(batch_size, to_num, n_heads*d_in))\n",
    "        return out, attn\n",
    "class SkipLayerNorm(nn.Module):\n",
    "    def __init__(self, to_len, to_dim):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm((to_len, to_dim))\n",
    "    def forward(self, x_0, x_1):\n",
    "        return self.layer_norm(x_0+x_1)\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, to_dim, hidden_dim, dropout_rate = 0.2):\n",
    "        super().__init__()\n",
    "        self.FFN = nn.Sequential(\n",
    "            nn.Linear(to_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, to_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.FFN(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651d3c2",
   "metadata": {},
   "source": [
    "Now, we can build the TransformerBlock thanks to the TensorDictModule. Since the changes affect the tensor dict, we just need to map outputs to the right name such as it is picked up by the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afd16e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlockTensorDict(TensorDictSequence):\n",
    "    def __init__(self, to_name, from_name, to_dim, to_len, from_dim, latent_dim, num_heads):\n",
    "        super().__init__(\n",
    "            TensorDictModule(TokensToQKV(to_dim, from_dim, latent_dim), in_keys=[to_name, from_name], out_keys=[\"Q\", \"K\", \"V\"]),\n",
    "            TensorDictModule(SplitHeads(num_heads), in_keys=[\"Q\", \"K\", \"V\"], out_keys=[\"Q\", \"K\", \"V\"]),\n",
    "            TensorDictModule(Attention(latent_dim, to_dim), in_keys=[\"Q\", \"K\", \"V\"], out_keys=[\"X_out\",\"Attn\"]),\n",
    "            TensorDictModule(SkipLayerNorm(to_len, to_dim), in_keys=[to_name, \"X_out\"], out_keys=[to_name]),\n",
    "        )\n",
    "class TransformerBlockEncoderTensorDict(TensorDictSequence):\n",
    "    def __init__(self, to_name, from_name, to_dim, to_len, from_dim, latent_dim, num_heads):\n",
    "        super().__init__(\n",
    "            AttentionBlockTensorDict(to_name, from_name, to_dim, to_len, from_dim, latent_dim, num_heads),\n",
    "            TensorDictModule(FFN(to_dim, 4*to_dim), in_keys=[to_name], out_keys=[\"X_out\"]),\n",
    "            TensorDictModule(SkipLayerNorm(to_len, to_dim), in_keys=[to_name, \"X_out\"], out_keys=[to_name]),\n",
    "        )\n",
    "class TransformerBlockDecoderTensorDict(TensorDictSequence):\n",
    "    def __init__(self, to_name, from_name, to_dim, to_len, from_dim, latent_dim, num_heads):\n",
    "        super().__init__(\n",
    "            AttentionBlockTensorDict(to_name, to_name, to_dim, to_len, to_dim, latent_dim, num_heads),\n",
    "            TransformerBlockEncoderTensorDict(to_name, from_name, to_dim, to_len, from_dim, latent_dim, num_heads)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eba41ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        Attn: Tensor(torch.Size([8, 2, 3, 10]), dtype=torch.float32),\n",
       "        K: Tensor(torch.Size([8, 2, 10, 5]), dtype=torch.float32),\n",
       "        Q: Tensor(torch.Size([8, 2, 3, 5]), dtype=torch.float32),\n",
       "        V: Tensor(torch.Size([8, 2, 10, 5]), dtype=torch.float32),\n",
       "        X_from: Tensor(torch.Size([8, 10, 6]), dtype=torch.float32),\n",
       "        X_out: Tensor(torch.Size([8, 3, 5]), dtype=torch.float32),\n",
       "        X_to: Tensor(torch.Size([8, 3, 5]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([8]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dim = 5\n",
    "from_dim = 6\n",
    "latent_dim = 10\n",
    "to_len = 3\n",
    "from_len = 10\n",
    "batch_size = 8\n",
    "num_heads = 2\n",
    "\n",
    "tokens = TensorDict(\n",
    "    {\n",
    "        \"X_to\": torch.randn(batch_size, to_len, to_dim),\n",
    "        \"X_from\": torch.randn(batch_size, from_len, from_dim)\n",
    "    },\n",
    "    batch_size=[batch_size]\n",
    ")\n",
    "\n",
    "transformer_block = AttentionBlockTensorDict(\n",
    "    \"X_to\",\n",
    "    \"X_from\",\n",
    "    to_dim,\n",
    "    to_len,\n",
    "    from_dim,\n",
    "    latent_dim,\n",
    "    num_heads\n",
    ")\n",
    "\n",
    "transformer_block(tokens)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c3eea",
   "metadata": {},
   "source": [
    "The output of the transformer layer can now be found at tokens[\"X_to\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb06a532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2024, -1.0720,  0.0880,  0.1581,  1.3610],\n",
       "         [-0.5052, -1.8012,  0.7504, -0.6252, -1.9845],\n",
       "         [ 0.7721,  0.9715,  0.5495,  1.2725,  0.2672]],\n",
       "\n",
       "        [[ 0.5293, -0.4636, -1.3908, -0.7254,  0.7734],\n",
       "         [-1.7271,  1.1405,  0.5576, -0.3882,  0.0643],\n",
       "         [ 0.6418,  1.0622,  1.1772, -1.8478,  0.5967]],\n",
       "\n",
       "        [[ 0.4203,  0.1880, -1.6405, -1.6656,  0.4852],\n",
       "         [ 0.5613,  0.6962,  0.3396,  0.1042,  1.3068],\n",
       "         [-0.0636, -0.3054,  1.7350, -1.7914, -0.3700]],\n",
       "\n",
       "        [[-0.6542,  0.3826, -0.9735,  1.6878, -0.2295],\n",
       "         [-0.6227,  0.1929,  1.3043,  1.3246, -1.2593],\n",
       "         [ 0.7568, -0.5468, -1.7795, -0.4934,  0.9099]],\n",
       "\n",
       "        [[ 0.6101,  1.1662, -0.1247, -0.0322,  0.3963],\n",
       "         [-1.3019,  1.8116, -0.8462,  0.8816, -1.8484],\n",
       "         [ 0.5544, -0.6557, -1.4419,  0.5660,  0.2648]],\n",
       "\n",
       "        [[-0.4518,  1.6725, -1.2902, -0.8343,  0.7091],\n",
       "         [-1.1318, -0.3141, -0.1082,  0.5590,  0.6859],\n",
       "         [ 0.7135,  1.9085,  0.0153, -1.5834, -0.5498]],\n",
       "\n",
       "        [[ 1.4865, -0.3687, -0.8707,  0.4826,  0.2675],\n",
       "         [ 0.6597,  0.9290, -1.1941, -0.9715, -1.8818],\n",
       "         [ 1.4414,  0.7599, -0.5614, -0.9248,  0.7463]],\n",
       "\n",
       "        [[ 0.3178, -0.6035,  1.0795,  0.4479,  0.5360],\n",
       "         [ 1.3815,  0.2269, -0.1231, -2.4940, -0.1504],\n",
       "         [ 1.2107,  0.1268,  0.2812, -1.6662, -0.5712]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"X_to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e1c95c",
   "metadata": {},
   "source": [
    "We can now create a transformer easily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5125af26",
   "metadata": {},
   "source": [
    "For an encoder, we just need to take the same tokens for both queries, keys and values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f909b289",
   "metadata": {},
   "source": [
    "For a decoder, we now can extract info from X_from into X_to. X_from will map to queries whereas X_from will map to keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b9f19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads\n",
    "    ):\n",
    "        super().__init__(*[TransformerBlockEncoderTensorDict(to_name, from_name, to_dim, to_len, from_dim, latent_dim, num_heads) for _ in range(num_blocks)])\n",
    "class TransformerDecoderTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads\n",
    "    ):\n",
    "        super().__init__(*[TransformerBlockDecoderTensorDict(to_name, from_name, to_dim, to_len, from_dim, latent_dim, num_heads) for _ in range(num_blocks)])\n",
    "        \n",
    "class TransformerTensorDict(TensorDictSequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks,\n",
    "        to_name,\n",
    "        from_name,\n",
    "        to_dim,\n",
    "        to_len,\n",
    "        from_dim,\n",
    "        latent_dim,\n",
    "        num_heads\n",
    "    ):\n",
    "        super().__init__(\n",
    "            TransformerEncoderTensorDict(\n",
    "                num_blocks,\n",
    "                to_name,\n",
    "                to_name,\n",
    "                to_dim,\n",
    "                to_len,\n",
    "                to_dim,\n",
    "                latent_dim,\n",
    "                num_heads\n",
    "            ),\n",
    "            TransformerDecoderTensorDict(\n",
    "                num_blocks,\n",
    "                from_name,\n",
    "                to_name,\n",
    "                from_dim,\n",
    "                from_len,\n",
    "                to_dim,\n",
    "                latent_dim,\n",
    "                num_heads\n",
    "            )\n",
    "            \n",
    "        )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e22913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dim = 5\n",
    "from_dim = 6\n",
    "latent_dim = 10\n",
    "to_len = 3\n",
    "from_len = 10\n",
    "batch_size = 8\n",
    "num_heads = 2\n",
    "\n",
    "tokens = TensorDict(\n",
    "    {\n",
    "        \"X_encode\":torch.randn(batch_size, to_len, to_dim),\n",
    "        \"X_decode\":torch.randn(batch_size, from_len, from_dim)\n",
    "    },\n",
    "    batch_size=[batch_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7b99195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        Attn: Tensor(torch.Size([8, 2, 10, 3]), dtype=torch.float32),\n",
       "        K: Tensor(torch.Size([8, 2, 3, 5]), dtype=torch.float32),\n",
       "        Q: Tensor(torch.Size([8, 2, 10, 5]), dtype=torch.float32),\n",
       "        V: Tensor(torch.Size([8, 2, 3, 5]), dtype=torch.float32),\n",
       "        X_from: Tensor(torch.Size([8, 10, 6]), dtype=torch.float32),\n",
       "        X_out: Tensor(torch.Size([8, 10, 6]), dtype=torch.float32),\n",
       "        X_to: Tensor(torch.Size([8, 3, 5]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([8]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer =  TransformerTensorDict(\n",
    "    6,\n",
    "    \"X_to\",\n",
    "    \"X_from\",\n",
    "    to_dim,\n",
    "    to_len,\n",
    "    from_dim,\n",
    "    latent_dim,\n",
    "    num_heads\n",
    ")\n",
    "\n",
    "transformer(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006a21d",
   "metadata": {},
   "source": [
    "Now we can look at the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db35f95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerTensorDict(\n",
       "    module=ModuleList(\n",
       "      (0): TransformerEncoderTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TransformerBlockEncoderTensorDict(\n",
       "                module=ModuleList(\n",
       "                  (0): AttentionBlockTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): TensorDictModule(\n",
       "                            module=TokensToQKV(\n",
       "                              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                              (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                              (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_to', 'X_to'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=SplitHeads(), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=Attention(\n",
       "                              (softmax): Softmax(dim=-1)\n",
       "                              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['X_out', 'Attn'])\n",
       "                        (3): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_to', 'X_out'], \n",
       "                            out_keys=['X_to'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to', 'X_to'], \n",
       "                      out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_to'])\n",
       "                  (1): TensorDictModule(\n",
       "                      module=FFN(\n",
       "                        (FFN): Sequential(\n",
       "                          (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                          (1): ReLU()\n",
       "                          (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                          (3): Dropout(p=0.2, inplace=False)\n",
       "                        )\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to'], \n",
       "                      out_keys=['X_out'])\n",
       "                  (2): TensorDictModule(\n",
       "                      module=SkipLayerNorm(\n",
       "                        (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to', 'X_out'], \n",
       "                      out_keys=['X_to'])\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "            (1): TransformerBlockEncoderTensorDict(\n",
       "                module=ModuleList(\n",
       "                  (0): AttentionBlockTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): TensorDictModule(\n",
       "                            module=TokensToQKV(\n",
       "                              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                              (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                              (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_to', 'X_to'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=SplitHeads(), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=Attention(\n",
       "                              (softmax): Softmax(dim=-1)\n",
       "                              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['X_out', 'Attn'])\n",
       "                        (3): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_to', 'X_out'], \n",
       "                            out_keys=['X_to'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to', 'X_to'], \n",
       "                      out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_to'])\n",
       "                  (1): TensorDictModule(\n",
       "                      module=FFN(\n",
       "                        (FFN): Sequential(\n",
       "                          (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                          (1): ReLU()\n",
       "                          (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                          (3): Dropout(p=0.2, inplace=False)\n",
       "                        )\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to'], \n",
       "                      out_keys=['X_out'])\n",
       "                  (2): TensorDictModule(\n",
       "                      module=SkipLayerNorm(\n",
       "                        (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to', 'X_out'], \n",
       "                      out_keys=['X_to'])\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "            (2): TransformerBlockEncoderTensorDict(\n",
       "                module=ModuleList(\n",
       "                  (0): AttentionBlockTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): TensorDictModule(\n",
       "                            module=TokensToQKV(\n",
       "                              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                              (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                              (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_to', 'X_to'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=SplitHeads(), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=Attention(\n",
       "                              (softmax): Softmax(dim=-1)\n",
       "                              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['X_out', 'Attn'])\n",
       "                        (3): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_to', 'X_out'], \n",
       "                            out_keys=['X_to'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to', 'X_to'], \n",
       "                      out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_to'])\n",
       "                  (1): TensorDictModule(\n",
       "                      module=FFN(\n",
       "                        (FFN): Sequential(\n",
       "                          (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                          (1): ReLU()\n",
       "                          (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                          (3): Dropout(p=0.2, inplace=False)\n",
       "                        )\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to'], \n",
       "                      out_keys=['X_out'])\n",
       "                  (2): TensorDictModule(\n",
       "                      module=SkipLayerNorm(\n",
       "                        (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to', 'X_out'], \n",
       "                      out_keys=['X_to'])\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "            (3): TransformerBlockEncoderTensorDict(\n",
       "                module=ModuleList(\n",
       "                  (0): AttentionBlockTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): TensorDictModule(\n",
       "                            module=TokensToQKV(\n",
       "                              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                              (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                              (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_to', 'X_to'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=SplitHeads(), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=Attention(\n",
       "                              (softmax): Softmax(dim=-1)\n",
       "                              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['X_out', 'Attn'])\n",
       "                        (3): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_to', 'X_out'], \n",
       "                            out_keys=['X_to'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to', 'X_to'], \n",
       "                      out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_to'])\n",
       "                  (1): TensorDictModule(\n",
       "                      module=FFN(\n",
       "                        (FFN): Sequential(\n",
       "                          (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                          (1): ReLU()\n",
       "                          (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                          (3): Dropout(p=0.2, inplace=False)\n",
       "                        )\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to'], \n",
       "                      out_keys=['X_out'])\n",
       "                  (2): TensorDictModule(\n",
       "                      module=SkipLayerNorm(\n",
       "                        (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to', 'X_out'], \n",
       "                      out_keys=['X_to'])\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "            (4): TransformerBlockEncoderTensorDict(\n",
       "                module=ModuleList(\n",
       "                  (0): AttentionBlockTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): TensorDictModule(\n",
       "                            module=TokensToQKV(\n",
       "                              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                              (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                              (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_to', 'X_to'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=SplitHeads(), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=Attention(\n",
       "                              (softmax): Softmax(dim=-1)\n",
       "                              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['X_out', 'Attn'])\n",
       "                        (3): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_to', 'X_out'], \n",
       "                            out_keys=['X_to'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to', 'X_to'], \n",
       "                      out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_to'])\n",
       "                  (1): TensorDictModule(\n",
       "                      module=FFN(\n",
       "                        (FFN): Sequential(\n",
       "                          (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                          (1): ReLU()\n",
       "                          (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                          (3): Dropout(p=0.2, inplace=False)\n",
       "                        )\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to'], \n",
       "                      out_keys=['X_out'])\n",
       "                  (2): TensorDictModule(\n",
       "                      module=SkipLayerNorm(\n",
       "                        (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to', 'X_out'], \n",
       "                      out_keys=['X_to'])\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "            (5): TransformerBlockEncoderTensorDict(\n",
       "                module=ModuleList(\n",
       "                  (0): AttentionBlockTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): TensorDictModule(\n",
       "                            module=TokensToQKV(\n",
       "                              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                              (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                              (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_to', 'X_to'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=SplitHeads(), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=Attention(\n",
       "                              (softmax): Softmax(dim=-1)\n",
       "                              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['X_out', 'Attn'])\n",
       "                        (3): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_to', 'X_out'], \n",
       "                            out_keys=['X_to'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to', 'X_to'], \n",
       "                      out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_to'])\n",
       "                  (1): TensorDictModule(\n",
       "                      module=FFN(\n",
       "                        (FFN): Sequential(\n",
       "                          (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                          (1): ReLU()\n",
       "                          (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                          (3): Dropout(p=0.2, inplace=False)\n",
       "                        )\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to'], \n",
       "                      out_keys=['X_out'])\n",
       "                  (2): TensorDictModule(\n",
       "                      module=SkipLayerNorm(\n",
       "                        (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_to', 'X_out'], \n",
       "                      out_keys=['X_to'])\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (1): TransformerDecoderTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TransformerBlockDecoderTensorDict(\n",
       "                module=ModuleList(\n",
       "                  (0): AttentionBlockTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): TensorDictModule(\n",
       "                            module=TokensToQKV(\n",
       "                              (q): Linear(in_features=6, out_features=10, bias=True)\n",
       "                              (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                              (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_from'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=SplitHeads(), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=Attention(\n",
       "                              (softmax): Softmax(dim=-1)\n",
       "                              (out): Linear(in_features=10, out_features=6, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['X_out', 'Attn'])\n",
       "                        (3): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_out'], \n",
       "                            out_keys=['X_from'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_from', 'X_from'], \n",
       "                      out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_from'])\n",
       "                  (1): TransformerBlockEncoderTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): AttentionBlockTensorDict(\n",
       "                            module=ModuleList(\n",
       "                              (0): TensorDictModule(\n",
       "                                  module=TokensToQKV(\n",
       "                                    (q): Linear(in_features=6, out_features=10, bias=True)\n",
       "                                    (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                                    (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['X_from', 'X_to'], \n",
       "                                  out_keys=['Q', 'K', 'V'])\n",
       "                              (1): TensorDictModule(\n",
       "                                  module=SplitHeads(), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['Q', 'K', 'V'], \n",
       "                                  out_keys=['Q', 'K', 'V'])\n",
       "                              (2): TensorDictModule(\n",
       "                                  module=Attention(\n",
       "                                    (softmax): Softmax(dim=-1)\n",
       "                                    (out): Linear(in_features=10, out_features=6, bias=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['Q', 'K', 'V'], \n",
       "                                  out_keys=['X_out', 'Attn'])\n",
       "                              (3): TensorDictModule(\n",
       "                                  module=SkipLayerNorm(\n",
       "                                    (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['X_from', 'X_out'], \n",
       "                                  out_keys=['X_from'])\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_to'], \n",
       "                            out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_from'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=FFN(\n",
       "                              (FFN): Sequential(\n",
       "                                (0): Linear(in_features=6, out_features=24, bias=True)\n",
       "                                (1): ReLU()\n",
       "                                (2): Linear(in_features=24, out_features=6, bias=True)\n",
       "                                (3): Dropout(p=0.2, inplace=False)\n",
       "                              )\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from'], \n",
       "                            out_keys=['X_out'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_out'], \n",
       "                            out_keys=['X_from'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_from', 'X_to'], \n",
       "                      out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_from', 'X_from', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "            (1): TransformerBlockDecoderTensorDict(\n",
       "                module=ModuleList(\n",
       "                  (0): AttentionBlockTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): TensorDictModule(\n",
       "                            module=TokensToQKV(\n",
       "                              (q): Linear(in_features=6, out_features=10, bias=True)\n",
       "                              (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                              (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_from'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=SplitHeads(), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=Attention(\n",
       "                              (softmax): Softmax(dim=-1)\n",
       "                              (out): Linear(in_features=10, out_features=6, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['X_out', 'Attn'])\n",
       "                        (3): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_out'], \n",
       "                            out_keys=['X_from'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_from', 'X_from'], \n",
       "                      out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_from'])\n",
       "                  (1): TransformerBlockEncoderTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): AttentionBlockTensorDict(\n",
       "                            module=ModuleList(\n",
       "                              (0): TensorDictModule(\n",
       "                                  module=TokensToQKV(\n",
       "                                    (q): Linear(in_features=6, out_features=10, bias=True)\n",
       "                                    (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                                    (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['X_from', 'X_to'], \n",
       "                                  out_keys=['Q', 'K', 'V'])\n",
       "                              (1): TensorDictModule(\n",
       "                                  module=SplitHeads(), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['Q', 'K', 'V'], \n",
       "                                  out_keys=['Q', 'K', 'V'])\n",
       "                              (2): TensorDictModule(\n",
       "                                  module=Attention(\n",
       "                                    (softmax): Softmax(dim=-1)\n",
       "                                    (out): Linear(in_features=10, out_features=6, bias=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['Q', 'K', 'V'], \n",
       "                                  out_keys=['X_out', 'Attn'])\n",
       "                              (3): TensorDictModule(\n",
       "                                  module=SkipLayerNorm(\n",
       "                                    (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['X_from', 'X_out'], \n",
       "                                  out_keys=['X_from'])\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_to'], \n",
       "                            out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_from'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=FFN(\n",
       "                              (FFN): Sequential(\n",
       "                                (0): Linear(in_features=6, out_features=24, bias=True)\n",
       "                                (1): ReLU()\n",
       "                                (2): Linear(in_features=24, out_features=6, bias=True)\n",
       "                                (3): Dropout(p=0.2, inplace=False)\n",
       "                              )\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from'], \n",
       "                            out_keys=['X_out'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_out'], \n",
       "                            out_keys=['X_from'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_from', 'X_to'], \n",
       "                      out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_from', 'X_from', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "            (2): TransformerBlockDecoderTensorDict(\n",
       "                module=ModuleList(\n",
       "                  (0): AttentionBlockTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): TensorDictModule(\n",
       "                            module=TokensToQKV(\n",
       "                              (q): Linear(in_features=6, out_features=10, bias=True)\n",
       "                              (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                              (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_from'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=SplitHeads(), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=Attention(\n",
       "                              (softmax): Softmax(dim=-1)\n",
       "                              (out): Linear(in_features=10, out_features=6, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['X_out', 'Attn'])\n",
       "                        (3): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_out'], \n",
       "                            out_keys=['X_from'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_from', 'X_from'], \n",
       "                      out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_from'])\n",
       "                  (1): TransformerBlockEncoderTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): AttentionBlockTensorDict(\n",
       "                            module=ModuleList(\n",
       "                              (0): TensorDictModule(\n",
       "                                  module=TokensToQKV(\n",
       "                                    (q): Linear(in_features=6, out_features=10, bias=True)\n",
       "                                    (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                                    (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['X_from', 'X_to'], \n",
       "                                  out_keys=['Q', 'K', 'V'])\n",
       "                              (1): TensorDictModule(\n",
       "                                  module=SplitHeads(), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['Q', 'K', 'V'], \n",
       "                                  out_keys=['Q', 'K', 'V'])\n",
       "                              (2): TensorDictModule(\n",
       "                                  module=Attention(\n",
       "                                    (softmax): Softmax(dim=-1)\n",
       "                                    (out): Linear(in_features=10, out_features=6, bias=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['Q', 'K', 'V'], \n",
       "                                  out_keys=['X_out', 'Attn'])\n",
       "                              (3): TensorDictModule(\n",
       "                                  module=SkipLayerNorm(\n",
       "                                    (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['X_from', 'X_out'], \n",
       "                                  out_keys=['X_from'])\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_to'], \n",
       "                            out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_from'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=FFN(\n",
       "                              (FFN): Sequential(\n",
       "                                (0): Linear(in_features=6, out_features=24, bias=True)\n",
       "                                (1): ReLU()\n",
       "                                (2): Linear(in_features=24, out_features=6, bias=True)\n",
       "                                (3): Dropout(p=0.2, inplace=False)\n",
       "                              )\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from'], \n",
       "                            out_keys=['X_out'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_out'], \n",
       "                            out_keys=['X_from'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_from', 'X_to'], \n",
       "                      out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_from', 'X_from', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "            (3): TransformerBlockDecoderTensorDict(\n",
       "                module=ModuleList(\n",
       "                  (0): AttentionBlockTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): TensorDictModule(\n",
       "                            module=TokensToQKV(\n",
       "                              (q): Linear(in_features=6, out_features=10, bias=True)\n",
       "                              (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                              (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_from'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=SplitHeads(), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=Attention(\n",
       "                              (softmax): Softmax(dim=-1)\n",
       "                              (out): Linear(in_features=10, out_features=6, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['X_out', 'Attn'])\n",
       "                        (3): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_out'], \n",
       "                            out_keys=['X_from'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_from', 'X_from'], \n",
       "                      out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_from'])\n",
       "                  (1): TransformerBlockEncoderTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): AttentionBlockTensorDict(\n",
       "                            module=ModuleList(\n",
       "                              (0): TensorDictModule(\n",
       "                                  module=TokensToQKV(\n",
       "                                    (q): Linear(in_features=6, out_features=10, bias=True)\n",
       "                                    (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                                    (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['X_from', 'X_to'], \n",
       "                                  out_keys=['Q', 'K', 'V'])\n",
       "                              (1): TensorDictModule(\n",
       "                                  module=SplitHeads(), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['Q', 'K', 'V'], \n",
       "                                  out_keys=['Q', 'K', 'V'])\n",
       "                              (2): TensorDictModule(\n",
       "                                  module=Attention(\n",
       "                                    (softmax): Softmax(dim=-1)\n",
       "                                    (out): Linear(in_features=10, out_features=6, bias=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['Q', 'K', 'V'], \n",
       "                                  out_keys=['X_out', 'Attn'])\n",
       "                              (3): TensorDictModule(\n",
       "                                  module=SkipLayerNorm(\n",
       "                                    (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['X_from', 'X_out'], \n",
       "                                  out_keys=['X_from'])\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_to'], \n",
       "                            out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_from'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=FFN(\n",
       "                              (FFN): Sequential(\n",
       "                                (0): Linear(in_features=6, out_features=24, bias=True)\n",
       "                                (1): ReLU()\n",
       "                                (2): Linear(in_features=24, out_features=6, bias=True)\n",
       "                                (3): Dropout(p=0.2, inplace=False)\n",
       "                              )\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from'], \n",
       "                            out_keys=['X_out'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_out'], \n",
       "                            out_keys=['X_from'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_from', 'X_to'], \n",
       "                      out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_from', 'X_from', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "            (4): TransformerBlockDecoderTensorDict(\n",
       "                module=ModuleList(\n",
       "                  (0): AttentionBlockTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): TensorDictModule(\n",
       "                            module=TokensToQKV(\n",
       "                              (q): Linear(in_features=6, out_features=10, bias=True)\n",
       "                              (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                              (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_from'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=SplitHeads(), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=Attention(\n",
       "                              (softmax): Softmax(dim=-1)\n",
       "                              (out): Linear(in_features=10, out_features=6, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['X_out', 'Attn'])\n",
       "                        (3): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_out'], \n",
       "                            out_keys=['X_from'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_from', 'X_from'], \n",
       "                      out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_from'])\n",
       "                  (1): TransformerBlockEncoderTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): AttentionBlockTensorDict(\n",
       "                            module=ModuleList(\n",
       "                              (0): TensorDictModule(\n",
       "                                  module=TokensToQKV(\n",
       "                                    (q): Linear(in_features=6, out_features=10, bias=True)\n",
       "                                    (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                                    (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['X_from', 'X_to'], \n",
       "                                  out_keys=['Q', 'K', 'V'])\n",
       "                              (1): TensorDictModule(\n",
       "                                  module=SplitHeads(), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['Q', 'K', 'V'], \n",
       "                                  out_keys=['Q', 'K', 'V'])\n",
       "                              (2): TensorDictModule(\n",
       "                                  module=Attention(\n",
       "                                    (softmax): Softmax(dim=-1)\n",
       "                                    (out): Linear(in_features=10, out_features=6, bias=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['Q', 'K', 'V'], \n",
       "                                  out_keys=['X_out', 'Attn'])\n",
       "                              (3): TensorDictModule(\n",
       "                                  module=SkipLayerNorm(\n",
       "                                    (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['X_from', 'X_out'], \n",
       "                                  out_keys=['X_from'])\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_to'], \n",
       "                            out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_from'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=FFN(\n",
       "                              (FFN): Sequential(\n",
       "                                (0): Linear(in_features=6, out_features=24, bias=True)\n",
       "                                (1): ReLU()\n",
       "                                (2): Linear(in_features=24, out_features=6, bias=True)\n",
       "                                (3): Dropout(p=0.2, inplace=False)\n",
       "                              )\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from'], \n",
       "                            out_keys=['X_out'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_out'], \n",
       "                            out_keys=['X_from'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_from', 'X_to'], \n",
       "                      out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_from', 'X_from', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "            (5): TransformerBlockDecoderTensorDict(\n",
       "                module=ModuleList(\n",
       "                  (0): AttentionBlockTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): TensorDictModule(\n",
       "                            module=TokensToQKV(\n",
       "                              (q): Linear(in_features=6, out_features=10, bias=True)\n",
       "                              (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                              (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_from'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=SplitHeads(), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['Q', 'K', 'V'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=Attention(\n",
       "                              (softmax): Softmax(dim=-1)\n",
       "                              (out): Linear(in_features=10, out_features=6, bias=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['Q', 'K', 'V'], \n",
       "                            out_keys=['X_out', 'Attn'])\n",
       "                        (3): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_out'], \n",
       "                            out_keys=['X_from'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_from', 'X_from'], \n",
       "                      out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_from'])\n",
       "                  (1): TransformerBlockEncoderTensorDict(\n",
       "                      module=ModuleList(\n",
       "                        (0): AttentionBlockTensorDict(\n",
       "                            module=ModuleList(\n",
       "                              (0): TensorDictModule(\n",
       "                                  module=TokensToQKV(\n",
       "                                    (q): Linear(in_features=6, out_features=10, bias=True)\n",
       "                                    (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "                                    (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['X_from', 'X_to'], \n",
       "                                  out_keys=['Q', 'K', 'V'])\n",
       "                              (1): TensorDictModule(\n",
       "                                  module=SplitHeads(), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['Q', 'K', 'V'], \n",
       "                                  out_keys=['Q', 'K', 'V'])\n",
       "                              (2): TensorDictModule(\n",
       "                                  module=Attention(\n",
       "                                    (softmax): Softmax(dim=-1)\n",
       "                                    (out): Linear(in_features=10, out_features=6, bias=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['Q', 'K', 'V'], \n",
       "                                  out_keys=['X_out', 'Attn'])\n",
       "                              (3): TensorDictModule(\n",
       "                                  module=SkipLayerNorm(\n",
       "                                    (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                                  ), \n",
       "                                  device=cpu, \n",
       "                                  in_keys=['X_from', 'X_out'], \n",
       "                                  out_keys=['X_from'])\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_to'], \n",
       "                            out_keys=['Q', 'K', 'V', 'X_out', 'Attn', 'X_from'])\n",
       "                        (1): TensorDictModule(\n",
       "                            module=FFN(\n",
       "                              (FFN): Sequential(\n",
       "                                (0): Linear(in_features=6, out_features=24, bias=True)\n",
       "                                (1): ReLU()\n",
       "                                (2): Linear(in_features=24, out_features=6, bias=True)\n",
       "                                (3): Dropout(p=0.2, inplace=False)\n",
       "                              )\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from'], \n",
       "                            out_keys=['X_out'])\n",
       "                        (2): TensorDictModule(\n",
       "                            module=SkipLayerNorm(\n",
       "                              (layer_norm): LayerNorm((10, 6), eps=1e-05, elementwise_affine=True)\n",
       "                            ), \n",
       "                            device=cpu, \n",
       "                            in_keys=['X_from', 'X_out'], \n",
       "                            out_keys=['X_from'])\n",
       "                      ), \n",
       "                      device=cpu, \n",
       "                      in_keys=['X_from', 'X_to'], \n",
       "                      out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_from', 'X_from', 'X_to'], \n",
       "                out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_from', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])\n",
       "    ), \n",
       "    device=cpu, \n",
       "    in_keys=['X_to', 'X_to', 'X_from', 'X_from'], \n",
       "    out_keys=['X_to', 'Q', 'K', 'V', 'Attn', 'X_out', 'X_from'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a17dda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerTensorDict(\n",
       "    module=ModuleList(\n",
       "      (0): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_from'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (1): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_from'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (2): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_from'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (3): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_from'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (4): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_from'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "      (5): TransformerBlockTensorDict(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=TokensToQKV(\n",
       "                  (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "                  (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "                  (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_from'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (1): TensorDictModule(\n",
       "                module=SplitHeads(), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['Q', 'K', 'V'])\n",
       "            (2): TensorDictModule(\n",
       "                module=Attention(\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['Q', 'K', 'V'], \n",
       "                out_keys=['X_out', 'Attn'])\n",
       "            (3): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "            (4): TensorDictModule(\n",
       "                module=FFN(\n",
       "                  (FFN): Sequential(\n",
       "                    (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                    (3): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to'], \n",
       "                out_keys=['X_out'])\n",
       "            (5): TensorDictModule(\n",
       "                module=SkipLayerNorm(\n",
       "                  (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "                ), \n",
       "                device=cpu, \n",
       "                in_keys=['X_to', 'X_out'], \n",
       "                out_keys=['X_to'])\n",
       "          ), \n",
       "          device=cpu, \n",
       "          in_keys=['X_to', 'X_from', 'X_to'], \n",
       "          out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])\n",
       "    ), \n",
       "    device=cpu, \n",
       "    in_keys=['X_to', 'X_from', 'X_to', 'X_from', 'X_from', 'X_from', 'X_from', 'X_from'], \n",
       "    out_keys=['Q', 'K', 'V', 'Attn', 'X_out', 'X_to'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e42002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
