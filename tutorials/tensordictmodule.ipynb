{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf31fa1a",
   "metadata": {},
   "source": [
    "# The TensorDictModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7270f31e",
   "metadata": {},
   "source": [
    "Make sure to first read the tensordict tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1987c",
   "metadata": {},
   "source": [
    "How do we use the TensorDict it in pratice? We introduce the TensorDictModule. The TensorDictModule is an nn.Module that takes a TensorDict in his forward method. The user defines the keys that the module will take as an input and write the output in the same TensorDict at a given set of key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26ce6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import TensorDictModule, TensorDictSequence\n",
    "from torchrl.data import TensorDict\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf9e9e",
   "metadata": {},
   "source": [
    "### Example: Simple Linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1a71b",
   "metadata": {},
   "source": [
    "Let's imagine we have 2 entries Tensor dict, a and b and we only want to affect a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ddc935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        a_out: Tensor(torch.Size([5, 10]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3), \"b\": torch.randn(5, 4, 3)}, batch_size=[5])\n",
    "linear = TensorDictModule(nn.Linear(3, 10),in_keys=[\"a\"], out_keys=[\"a_out\"])\n",
    "linear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77864a9",
   "metadata": {},
   "source": [
    "We can also do it inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7cbec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 10]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3), \"b\": torch.randn(5, 4, 3)}, batch_size=[5])\n",
    "linear = TensorDictModule(nn.Linear(3, 10),in_keys=[\"a\"], out_keys=[\"a\"])\n",
    "linear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06f3be",
   "metadata": {},
   "source": [
    "### Example: 2 input merging with 2 linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb7628",
   "metadata": {},
   "source": [
    "Now lets imagine a more complex network that takes 2 entries and average them into a single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7843416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeLinear(nn.Module):\n",
    "    def __init__(self, in_1, in_2, out):\n",
    "        super().__init__()\n",
    "        self.linear_1  = nn.Linear(in_1,out)\n",
    "        self.linear_2  = nn.Linear(in_2,out)\n",
    "    def forward(self, x_1, x_2):\n",
    "        return (self.linear_1(x_1) + self.linear_2(x_2))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "250ea5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32),\n",
       "        c: Tensor(torch.Size([5, 4]), dtype=torch.float32),\n",
       "        output: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3), \"b\": torch.randn(5, 4, 3), \"c\":torch.randn(5, 4)}, batch_size=[5])\n",
    "mergelinear = TensorDictModule(MergeLinear(3, 4, 10),in_keys=[\"a\",\"c\"], out_keys=[\"output\"])\n",
    "mergelinear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59bba03",
   "metadata": {},
   "source": [
    "### Example: 1 input to 2 outputs linear layer\n",
    "We can also map to multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb948ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLinear(nn.Module):\n",
    "    def __init__(self, in_1, out_1, out_2):\n",
    "        super().__init__()\n",
    "        self.linear_1  = nn.Linear(in_1,out_1)\n",
    "        self.linear_2  = nn.Linear(in_1,out_2)\n",
    "    def forward(self, x):\n",
    "        return self.linear_1(x), self.linear_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544962c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        a: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
       "        b: Tensor(torch.Size([5, 4, 3]), dtype=torch.float32),\n",
       "        output_1: Tensor(torch.Size([5, 4]), dtype=torch.float32),\n",
       "        output_2: Tensor(torch.Size([5, 10]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensordict = TensorDict({\"a\": torch.randn(5, 3), \"b\": torch.randn(5, 4, 3)}, batch_size=[5])\n",
    "mergelinear = TensorDictModule(MultiHeadLinear(3, 4, 10),in_keys=[\"a\"], out_keys=[\"output_1\", \"output_2\"])\n",
    "mergelinear(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48405202",
   "metadata": {},
   "source": [
    "As we shown previously, the TensorDictModule can take any nn.Module and perform the operations inside a TensorDict. When having multiple input keys and output keys, make sure they match the order in the module.\n",
    "The tensordictmodule allows to use only the tensors that we want and keep the output inside the same object. It can even perform the operations inplace by setting the output key to be the same as an already set key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69ff38",
   "metadata": {},
   "source": [
    "### Example: A transformer with TensorDict?\n",
    "Let's attempt to create a transformer with TensorDict and TensorDictModule\n",
    "\n",
    "Disclaimer: This implementation don't claim to be \"better\" than a classical tensor-based implementation. It is just meant to showcase the TensorDictModule features.\n",
    "For simplicity we will not have positional encoders.\n",
    "\n",
    "Let's first implement the classical transformers blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b6278c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokensToQKV(nn.Module):\n",
    "    def __init__(self, to_dim, from_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(to_dim, latent_dim)\n",
    "        self.k = nn.Linear(from_dim, latent_dim)\n",
    "        self.v = nn.Linear(from_dim, latent_dim)\n",
    "    def forward(self, X_to, X_from):\n",
    "        Q = self.q(X_to)\n",
    "        K = self.k(X_from)\n",
    "        V = self.v(X_from)\n",
    "        return Q, K, V\n",
    "\n",
    "class SplitHeads(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size, to_num, latent_dim = Q.shape\n",
    "        _, from_num, _ = K.shape\n",
    "        d_tensor = latent_dim // self.num_heads\n",
    "        Q = Q.reshape(batch_size, to_num, self.num_heads, d_tensor).transpose(1, 2)\n",
    "        K = K.reshape(batch_size, from_num, self.num_heads, d_tensor).transpose(1, 2)\n",
    "        V = V.reshape(batch_size, from_num, self.num_heads, d_tensor).transpose(1, 2)\n",
    "        return Q, K, V\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, latent_dim, to_dim):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.out = nn.Linear(latent_dim, to_dim)\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size, n_heads, to_num, d_in = Q.shape\n",
    "        attn = self.softmax(Q @ K.transpose(2, 3) / d_in)\n",
    "        out = attn @ V\n",
    "        out = self.out(out.transpose(1, 2).reshape(batch_size, to_num, n_heads*d_in))\n",
    "        return out, attn\n",
    "class SkipLayerNorm(nn.Module):\n",
    "    def __init__(self, to_len, to_dim):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm((to_len, to_dim))\n",
    "    def forward(self, x_0, x_1):\n",
    "        return self.layer_norm(x_0+x_1)\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, to_dim, hidden_dim, dropout_rate = 0.2):\n",
    "        super().__init__()\n",
    "        self.FFN = nn.Sequential(\n",
    "            nn.Linear(to_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, to_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.FFN(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca50494",
   "metadata": {},
   "source": [
    "Now, we can build the TransformerBlock thanks to the TensorDictModule. Since the changes affect the tensor dict, we just need to map outputs to the right name such as it is picked up by the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da8fd12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockTensorDict(TensorDictSequence):\n",
    "    def __init__(self, to_name, from_name, to_dim, to_len, from_dim, latent_dim, num_heads):\n",
    "        super().__init__(\n",
    "            TensorDictModule(TokensToQKV(to_dim, from_dim, latent_dim), in_keys=[to_name, from_name], out_keys=[\"Q\", \"K\", \"V\"]),\n",
    "            TensorDictModule(SplitHeads(num_heads), in_keys=[\"Q\", \"K\", \"V\"], out_keys=[\"Q\", \"K\", \"V\"]),\n",
    "            TensorDictModule(Attention(latent_dim, to_dim), in_keys=[\"Q\", \"K\", \"V\"], out_keys=[\"X_out\",\"Attn\"]),\n",
    "            TensorDictModule(SkipLayerNorm(to_len, to_dim), in_keys=[\"X_to\", \"X_out\"], out_keys=[\"X_to\"]),\n",
    "            TensorDictModule(FFN(to_dim, 4*to_dim), in_keys=[\"X_to\"], out_keys=[\"X_out\"]),\n",
    "            TensorDictModule(SkipLayerNorm(to_len, to_dim), in_keys=[\"X_to\", \"X_out\"], out_keys=[\"X_to\"]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff60c29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        Attn: Tensor(torch.Size([8, 2, 3, 10]), dtype=torch.float32),\n",
       "        K: Tensor(torch.Size([8, 2, 10, 5]), dtype=torch.float32),\n",
       "        Q: Tensor(torch.Size([8, 2, 3, 5]), dtype=torch.float32),\n",
       "        V: Tensor(torch.Size([8, 2, 10, 5]), dtype=torch.float32),\n",
       "        X_from: Tensor(torch.Size([8, 10, 6]), dtype=torch.float32),\n",
       "        X_out: Tensor(torch.Size([8, 3, 5]), dtype=torch.float32),\n",
       "        X_to: Tensor(torch.Size([8, 3, 5]), dtype=torch.float32)},\n",
       "    batch_size=torch.Size([8]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dim = 5\n",
    "from_dim = 6\n",
    "latent_dim = 10\n",
    "to_len = 3\n",
    "from_len = 10\n",
    "batch_size = 8\n",
    "num_heads = 2\n",
    "\n",
    "tokens = TensorDict(\n",
    "    {\n",
    "        \"X_to\": torch.randn(batch_size, to_len, to_dim),\n",
    "        \"X_from\": torch.randn(batch_size, from_len, from_dim)\n",
    "    },\n",
    "    batch_size=[batch_size]\n",
    ")\n",
    "\n",
    "transformer_block = TransformerBlockTensorDict(\n",
    "    \"X_to\",\n",
    "    \"X_from\",\n",
    "    to_dim,\n",
    "    to_len,\n",
    "    from_dim,\n",
    "    latent_dim,\n",
    "    num_heads\n",
    ")\n",
    "\n",
    "transformer_block(tokens)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc34e6",
   "metadata": {},
   "source": [
    "The output of the transformer layer can now be found at tokens[\"X_to\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be55bd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8713, -1.2626,  1.3218, -0.2947,  1.6938],\n",
       "         [-0.7374, -0.6038, -1.4958, -0.3975,  0.0741],\n",
       "         [-0.5026,  0.0095,  1.8102,  0.2308,  1.0256]],\n",
       "\n",
       "        [[ 0.4484,  0.0689,  1.0152, -1.1690,  1.2264],\n",
       "         [-0.5891,  0.0737,  0.7038,  1.0404,  0.0131],\n",
       "         [ 1.0050, -2.4708, -0.7890, -1.0160,  0.4391]],\n",
       "\n",
       "        [[-1.8364, -0.5181, -0.5258,  0.5166,  1.8120],\n",
       "         [ 1.3389,  0.1451, -0.1267, -0.7637,  1.6104],\n",
       "         [-0.1859, -0.4134, -1.4359, -0.1131,  0.4961]],\n",
       "\n",
       "        [[-1.0511,  0.1636, -0.9440, -0.2152, -0.4874],\n",
       "         [ 1.4676,  2.0405,  0.2846,  0.5990,  1.0199],\n",
       "         [-1.5073,  0.0980, -1.5943, -0.1160,  0.2421]],\n",
       "\n",
       "        [[-0.6059,  0.3442,  0.6854, -0.0933,  1.8850],\n",
       "         [-0.2040, -2.0479,  0.8991,  1.1162,  0.0855],\n",
       "         [-1.6792, -0.4797,  0.4558,  0.4763, -0.8374]],\n",
       "\n",
       "        [[-1.2463, -1.3887,  1.2930,  0.5651,  0.9994],\n",
       "         [-0.1023, -0.4523, -2.0760,  1.6500,  0.5962],\n",
       "         [ 0.7221,  0.1171,  0.1437, -0.2839, -0.5374]],\n",
       "\n",
       "        [[-0.3637, -1.2232, -2.0972, -0.7830, -0.1663],\n",
       "         [-0.5369,  0.0789, -0.9869,  0.6352,  1.5502],\n",
       "         [ 0.3313,  0.2594,  1.5771,  0.6336,  1.0914]],\n",
       "\n",
       "        [[-1.0102,  0.4594,  0.5603,  0.1587,  0.2164],\n",
       "         [-1.3799, -0.1682, -1.9153,  0.9154,  1.8860],\n",
       "         [-0.0694, -0.8951, -0.6851,  0.6067,  1.3203]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"X_to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7084629a",
   "metadata": {},
   "source": [
    "We can now create a transformer easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85026014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTensorDict(nn.Module):\n",
    "    def __init__(self, num_blocks, to_name, from_name, to_dim, to_len, from_dim, latent_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.ModuleList([TransformerBlockTensorDict(to_name, from_name, to_dim, to_len, from_dim, latent_dim, num_heads) for _ in range(num_blocks)])\n",
    "    def forward(self, X_tensor_dict):\n",
    "        for transformer_block in self.transformer:\n",
    "            transformer_block(X_tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1355ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dim = 5\n",
    "from_dim = 6\n",
    "latent_dim = 10\n",
    "to_len = 3\n",
    "from_len = 10\n",
    "batch_size = 8\n",
    "num_heads = 2\n",
    "\n",
    "tokens = TensorDict({\"X_to\":torch.randn(batch_size, to_len, to_dim), \"X_from\":torch.randn(batch_size, from_len, from_dim)}, batch_size=[batch_size])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8c402",
   "metadata": {},
   "source": [
    "For an encoder, we just need to take the same tokens for both queries, keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ccbbf16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.0456e-01,  6.2688e-01, -1.1026e+00,  2.2786e-02, -4.8655e-02],\n",
       "         [-9.7513e-01,  1.4375e+00,  9.4504e-01,  2.0078e+00, -8.5485e-01],\n",
       "         [-1.3192e+00,  8.7940e-01, -1.2088e+00,  4.6428e-01, -1.7003e-01]],\n",
       "\n",
       "        [[-2.4683e+00,  1.1900e+00, -4.6999e-01,  8.1202e-01, -1.0019e+00],\n",
       "         [ 9.6744e-01,  1.0676e+00,  1.0292e+00,  4.7986e-01, -1.2177e+00],\n",
       "         [ 3.2124e-02,  4.6237e-01, -7.1896e-01,  2.0720e-03, -1.6574e-01]],\n",
       "\n",
       "        [[-2.2703e-01,  1.5631e+00,  1.1274e+00,  8.1163e-02, -1.5204e+00],\n",
       "         [-1.8939e+00,  7.4907e-01, -1.6144e+00,  7.3381e-01,  7.8596e-01],\n",
       "         [-7.7463e-02,  2.4682e-01,  4.6115e-01,  3.5791e-01, -7.7312e-01]],\n",
       "\n",
       "        [[ 4.2789e-01,  7.7004e-02, -5.2232e-01, -1.3905e+00, -6.8685e-01],\n",
       "         [-9.8940e-01,  6.9261e-02,  1.8176e+00,  1.2323e+00, -2.8591e-01],\n",
       "         [-1.9008e+00,  1.2424e+00,  9.0587e-01,  3.6788e-01, -3.6444e-01]],\n",
       "\n",
       "        [[ 1.4538e+00,  4.6922e-01, -8.1502e-01, -3.0426e-01,  3.3914e-01],\n",
       "         [ 4.5448e-02,  5.8241e-01, -5.1411e-02,  2.1194e-01, -3.5672e-01],\n",
       "         [-2.6675e+00,  1.0907e+00,  9.3493e-01,  4.4590e-01, -1.3785e+00]],\n",
       "\n",
       "        [[-8.6485e-01,  1.1146e+00, -9.4181e-02, -1.1407e-01,  6.2847e-01],\n",
       "         [-9.5162e-01,  1.4542e+00,  1.2157e-02, -2.0240e-01, -1.5317e+00],\n",
       "         [-9.3001e-01,  9.8748e-01,  9.6898e-01,  1.2264e+00, -1.7035e+00]],\n",
       "\n",
       "        [[ 1.8318e-01,  8.8817e-01, -4.1605e-01, -7.2077e-02,  1.0248e+00],\n",
       "         [ 1.1950e+00, -3.1990e-01, -3.0087e+00, -8.1052e-01,  4.2547e-01],\n",
       "         [ 4.8264e-02,  8.2594e-01, -4.8231e-01, -2.2883e-01,  7.4760e-01]],\n",
       "\n",
       "        [[-2.0185e+00,  2.7137e-01,  1.9260e-01, -3.7461e-01, -1.4131e+00],\n",
       "         [ 3.0497e-01,  4.7218e-01,  1.4051e+00, -7.2168e-01, -7.7026e-01],\n",
       "         [ 1.3967e+00, -6.0792e-01,  1.1108e-01,  1.7145e+00,  3.7664e-02]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder = TransformerTensorDict(6, \"X_to\", \"X_to\", to_dim, to_len, to_dim, latent_dim, num_heads)\n",
    "\n",
    "transformer_encoder(tokens)\n",
    "tokens[\"X_to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5fe38",
   "metadata": {},
   "source": [
    "For a decoder, we now can extract info from X_from into X_to. X_to will map to queries whereas X_from will map to keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16a72a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3380,  0.7571, -1.0049,  0.7433, -0.1268],\n",
       "         [-1.2908,  1.1133, -0.1441,  1.9060, -1.1740],\n",
       "         [-1.2169,  1.1862, -0.9880,  0.9077, -0.3301]],\n",
       "\n",
       "        [[-0.9989,  1.0225,  0.0516,  1.4268, -1.3924],\n",
       "         [-0.4546,  1.6237, -0.9002,  1.0320, -1.1606],\n",
       "         [-0.5801,  0.9752, -1.1864,  0.4610,  0.0803]],\n",
       "\n",
       "        [[-0.9055,  1.3019, -0.1604,  1.2191, -1.8533],\n",
       "         [-1.2209,  1.0449, -0.6953,  1.0392,  0.4056],\n",
       "         [-0.3828,  0.6094,  0.0309,  0.9272, -1.3600]],\n",
       "\n",
       "        [[-0.3067,  0.0189, -0.9604, -0.3966, -0.3716],\n",
       "         [-1.0679,  0.4198,  1.2470,  1.9662, -0.8475],\n",
       "         [-1.6165,  1.1001, -0.3376,  1.4791, -0.3264]],\n",
       "\n",
       "        [[-0.0980,  0.6583, -1.2214,  0.2261, -0.4485],\n",
       "         [-0.6073,  1.5547, -0.3092,  1.1512, -1.0601],\n",
       "         [-1.4279,  1.2677,  0.1266,  1.4727, -1.2850]],\n",
       "\n",
       "        [[-0.4164,  0.7499,  0.2536,  0.1846,  0.1112],\n",
       "         [-0.4129,  1.0208, -0.3762,  0.7487, -1.3215],\n",
       "         [-0.1598,  0.5029, -0.3288,  1.9525, -2.5087]],\n",
       "\n",
       "        [[-0.3682,  0.8304, -0.8471,  0.5485,  0.4438],\n",
       "         [-0.8762,  1.1699, -2.6552,  0.4682,  0.1896],\n",
       "         [-0.3391,  1.0842, -1.0764,  0.5564,  0.8712]],\n",
       "\n",
       "        [[-1.0284,  0.3804,  0.4793,  0.4096, -2.1200],\n",
       "         [-0.2695,  1.0064, -0.2066,  0.1593, -0.7439],\n",
       "         [ 0.9516,  0.8927, -0.9734,  1.9427, -0.8802]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_decoder = TransformerTensorDict(6, \"X_to\", \"X_from\", to_dim, to_len, from_dim, latent_dim, num_heads)\n",
    "\n",
    "transformer_decoder(tokens)\n",
    "tokens[\"X_to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571146e",
   "metadata": {},
   "source": [
    "Now we can look at both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42291d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerTensorDict(\n",
       "  (transformer): ModuleList(\n",
       "    (0): TransformerBlockTensorDict(\n",
       "      (transformer_block): Sequential(\n",
       "        (0): TensorDictModule(\n",
       "            module=TokensToQKV(\n",
       "              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_to'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (1): TensorDictModule(\n",
       "            module=SplitHeads(), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (2): TensorDictModule(\n",
       "            module=Attention(\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['X_out', 'Attn'])\n",
       "        (3): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "        (4): TensorDictModule(\n",
       "            module=FFN(\n",
       "              (FFN): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                (3): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to'], \n",
       "            out_keys=['X_out'])\n",
       "        (5): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlockTensorDict(\n",
       "      (transformer_block): Sequential(\n",
       "        (0): TensorDictModule(\n",
       "            module=TokensToQKV(\n",
       "              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_to'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (1): TensorDictModule(\n",
       "            module=SplitHeads(), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (2): TensorDictModule(\n",
       "            module=Attention(\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['X_out', 'Attn'])\n",
       "        (3): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "        (4): TensorDictModule(\n",
       "            module=FFN(\n",
       "              (FFN): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                (3): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to'], \n",
       "            out_keys=['X_out'])\n",
       "        (5): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlockTensorDict(\n",
       "      (transformer_block): Sequential(\n",
       "        (0): TensorDictModule(\n",
       "            module=TokensToQKV(\n",
       "              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_to'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (1): TensorDictModule(\n",
       "            module=SplitHeads(), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (2): TensorDictModule(\n",
       "            module=Attention(\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['X_out', 'Attn'])\n",
       "        (3): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "        (4): TensorDictModule(\n",
       "            module=FFN(\n",
       "              (FFN): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                (3): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to'], \n",
       "            out_keys=['X_out'])\n",
       "        (5): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlockTensorDict(\n",
       "      (transformer_block): Sequential(\n",
       "        (0): TensorDictModule(\n",
       "            module=TokensToQKV(\n",
       "              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_to'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (1): TensorDictModule(\n",
       "            module=SplitHeads(), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (2): TensorDictModule(\n",
       "            module=Attention(\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['X_out', 'Attn'])\n",
       "        (3): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "        (4): TensorDictModule(\n",
       "            module=FFN(\n",
       "              (FFN): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                (3): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to'], \n",
       "            out_keys=['X_out'])\n",
       "        (5): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlockTensorDict(\n",
       "      (transformer_block): Sequential(\n",
       "        (0): TensorDictModule(\n",
       "            module=TokensToQKV(\n",
       "              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_to'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (1): TensorDictModule(\n",
       "            module=SplitHeads(), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (2): TensorDictModule(\n",
       "            module=Attention(\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['X_out', 'Attn'])\n",
       "        (3): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "        (4): TensorDictModule(\n",
       "            module=FFN(\n",
       "              (FFN): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                (3): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to'], \n",
       "            out_keys=['X_out'])\n",
       "        (5): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlockTensorDict(\n",
       "      (transformer_block): Sequential(\n",
       "        (0): TensorDictModule(\n",
       "            module=TokensToQKV(\n",
       "              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (k): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (v): Linear(in_features=5, out_features=10, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_to'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (1): TensorDictModule(\n",
       "            module=SplitHeads(), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (2): TensorDictModule(\n",
       "            module=Attention(\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['X_out', 'Attn'])\n",
       "        (3): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "        (4): TensorDictModule(\n",
       "            module=FFN(\n",
       "              (FFN): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                (3): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to'], \n",
       "            out_keys=['X_out'])\n",
       "        (5): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d9af81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerTensorDict(\n",
       "  (transformer): ModuleList(\n",
       "    (0): TransformerBlockTensorDict(\n",
       "      (transformer_block): Sequential(\n",
       "        (0): TensorDictModule(\n",
       "            module=TokensToQKV(\n",
       "              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "              (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_from'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (1): TensorDictModule(\n",
       "            module=SplitHeads(), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (2): TensorDictModule(\n",
       "            module=Attention(\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['X_out', 'Attn'])\n",
       "        (3): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "        (4): TensorDictModule(\n",
       "            module=FFN(\n",
       "              (FFN): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                (3): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to'], \n",
       "            out_keys=['X_out'])\n",
       "        (5): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlockTensorDict(\n",
       "      (transformer_block): Sequential(\n",
       "        (0): TensorDictModule(\n",
       "            module=TokensToQKV(\n",
       "              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "              (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_from'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (1): TensorDictModule(\n",
       "            module=SplitHeads(), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (2): TensorDictModule(\n",
       "            module=Attention(\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['X_out', 'Attn'])\n",
       "        (3): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "        (4): TensorDictModule(\n",
       "            module=FFN(\n",
       "              (FFN): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                (3): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to'], \n",
       "            out_keys=['X_out'])\n",
       "        (5): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlockTensorDict(\n",
       "      (transformer_block): Sequential(\n",
       "        (0): TensorDictModule(\n",
       "            module=TokensToQKV(\n",
       "              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "              (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_from'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (1): TensorDictModule(\n",
       "            module=SplitHeads(), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (2): TensorDictModule(\n",
       "            module=Attention(\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['X_out', 'Attn'])\n",
       "        (3): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "        (4): TensorDictModule(\n",
       "            module=FFN(\n",
       "              (FFN): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                (3): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to'], \n",
       "            out_keys=['X_out'])\n",
       "        (5): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlockTensorDict(\n",
       "      (transformer_block): Sequential(\n",
       "        (0): TensorDictModule(\n",
       "            module=TokensToQKV(\n",
       "              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "              (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_from'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (1): TensorDictModule(\n",
       "            module=SplitHeads(), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (2): TensorDictModule(\n",
       "            module=Attention(\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['X_out', 'Attn'])\n",
       "        (3): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "        (4): TensorDictModule(\n",
       "            module=FFN(\n",
       "              (FFN): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                (3): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to'], \n",
       "            out_keys=['X_out'])\n",
       "        (5): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlockTensorDict(\n",
       "      (transformer_block): Sequential(\n",
       "        (0): TensorDictModule(\n",
       "            module=TokensToQKV(\n",
       "              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "              (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_from'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (1): TensorDictModule(\n",
       "            module=SplitHeads(), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (2): TensorDictModule(\n",
       "            module=Attention(\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['X_out', 'Attn'])\n",
       "        (3): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "        (4): TensorDictModule(\n",
       "            module=FFN(\n",
       "              (FFN): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                (3): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to'], \n",
       "            out_keys=['X_out'])\n",
       "        (5): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlockTensorDict(\n",
       "      (transformer_block): Sequential(\n",
       "        (0): TensorDictModule(\n",
       "            module=TokensToQKV(\n",
       "              (q): Linear(in_features=5, out_features=10, bias=True)\n",
       "              (k): Linear(in_features=6, out_features=10, bias=True)\n",
       "              (v): Linear(in_features=6, out_features=10, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_from'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (1): TensorDictModule(\n",
       "            module=SplitHeads(), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['Q', 'K', 'V'])\n",
       "        (2): TensorDictModule(\n",
       "            module=Attention(\n",
       "              (softmax): Softmax(dim=-1)\n",
       "              (out): Linear(in_features=10, out_features=5, bias=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['Q', 'K', 'V'], \n",
       "            out_keys=['X_out', 'Attn'])\n",
       "        (3): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "        (4): TensorDictModule(\n",
       "            module=FFN(\n",
       "              (FFN): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "                (3): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to'], \n",
       "            out_keys=['X_out'])\n",
       "        (5): TensorDictModule(\n",
       "            module=SkipLayerNorm(\n",
       "              (layer_norm): LayerNorm((3, 5), eps=1e-05, elementwise_affine=True)\n",
       "            ), \n",
       "            device=cpu, \n",
       "            in_keys=['X_to', 'X_out'], \n",
       "            out_keys=['X_to'])\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
